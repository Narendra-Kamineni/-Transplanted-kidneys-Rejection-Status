<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html><head></head><body>


































































<div class="container-fluid main-container">



<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row">

<div class="btn-group pull-right">
<span>Code</span> <span class="caret"></span>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a rel="noopener" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Project Analysis of High Dimensional Data</h1>
<h4 class="author">Bhanu Durganath Angam, Birgit Deboutte, Narendra Kamineni</h4>
<h4 class="date">2020-12-17</h4>

</div>


<div class="section level1">
<h1>Summary</h1>
<p>In order to find a possible relation between the rejection status of transplanted kidneys and gene expression we looked at a data set of 13699 genes for 250 patients. On visualizing the data we can see that there is on average a difference in gene expression between genes from a patient with rejected kidneys compared to no rejection. In order to explore this further, t-tests were performed for all 13699 genes to see if they were differentially expressed. Running so many tests simultaneously will cause a multiple testing problem, and will return many false positives. We set the false discovery rate at 10%, meaning that we accepted on average a 10% chance of the tests coming up with a false positive. After correcting for the multiple testing problem, we found 3106 genes to show differential expression. We filtered these results additionally using a local false discovery rate in order to find the most promising genes. We then had 522 genes with a very low probability of being false positives. Next, we tried to find a prediction model that could predict the rejection status from gene expression. The original data set was subset in a training set and a test set. We tried out three different prediction model building methods, using the training data set, obtained the optimal model for each of these methods and evaluated these models with the test data set in terms of sensitivity and specificity. The Lasso model gave the best results, giving us a prediction model with a sensitivity of 0.77 and a specificity of 0.81.</p>
</div>
<div class="section level1">
<h1>Data set</h1>
<p>The purpose of this project is to find a possible relation between the rejection status of transplanted kidneys and gene expression. We will work with a subset of 13669 genes for 25O patients, 174 of these patients did not reject their transplanted kidneys, 76 did reject them. The 13669 genes are the 25% most variable genes of a complete set of 54675 genes.</p>
</div>
<div class="section level1">
<h1>Data visualisation</h1>
<p>We will first try to visualize the data in order to see if certain patterns can be discovered.</p>
<div class="section level2">
<h2>Scree plot</h2>
<p>We perform an SVD on X:</p>
<p><span class="math display">\[
X = \sum_{k=1}^r\delta_ku_kv_k^T
\]</span> with <span class="math inline">\(r\)</span> the rank of X, <span class="math inline">\(\delta\)</span> the singular values, <span class="math inline">\(u\)</span> the left singular vectors and <span class="math inline">\(v\)</span> the right singular vectors.</p>
<p>We proceed to plot the singular values (principal components or PC’s) against the proportion of the total variance that they account for.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb1-1"></a>svdX &lt;-<span class="st"> </span><span class="kw">svd</span>(X)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb2-1"></a>nX &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span><a rel="noopener" href="#cb2-2"></a>r &lt;-<span class="st"> </span><span class="kw">ncol</span>(svdX<span class="op">$</span>v)</span>
<span><a rel="noopener" href="#cb2-3"></a></span>
<span><a rel="noopener" href="#cb2-4"></a>totVar &lt;-<span class="st"> </span><span class="kw">sum</span>(svdX<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(nX<span class="dv">-1</span>)</span>
<span><a rel="noopener" href="#cb2-5"></a>vars &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">comp=</span><span class="dv">1</span><span class="op">:</span>r,<span class="dt">var=</span>svdX<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(nX<span class="dv">-1</span>)) <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb2-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">propVar=</span>var<span class="op">/</span>totVar,<span class="dt">cumVar=</span><span class="kw">cumsum</span>(var<span class="op">/</span>totVar))</span>
<span><a rel="noopener" href="#cb2-7"></a></span>
<span><a rel="noopener" href="#cb2-8"></a>pVar2 &lt;-<span class="st"> </span>vars <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb2-9"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>comp<span class="op">:</span>r,<span class="dt">y=</span>propVar)) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-10"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-11"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-12"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;PC&quot;</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-13"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Proportion of Total Variance&quot;</span>)</span>
<span><a rel="noopener" href="#cb2-14"></a></span>
<span><a rel="noopener" href="#cb2-15"></a>pVar3 &lt;-<span class="st"> </span>vars <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb2-16"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>comp<span class="op">:</span>r,<span class="dt">y=</span>cumVar)) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-17"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-18"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-19"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;PC&quot;</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb2-20"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Cumulative Proportion of Total Variance&quot;</span>)</span>
<span><a rel="noopener" href="#cb2-21"></a></span>
<span><a rel="noopener" href="#cb2-22"></a><span class="kw">grid.arrange</span>(pVar2, pVar3, <span class="dt">nrow=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>As we can see, even the first PC’s don’t explain much of the variability, which means we need many PC’s to explain most of the variability.</p>
<p>Nevertheless, the first 2 principal components are used in order to facilitate visualisation of the data.</p>
</div>
<div class="section level2">
<h2>Scatterplot</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb3-1"></a>k &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span><a rel="noopener" href="#cb3-2"></a>Vk &lt;-<span class="st"> </span>svdX<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span><a rel="noopener" href="#cb3-3"></a>Uk &lt;-<span class="st"> </span>svdX<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span><a rel="noopener" href="#cb3-4"></a>Dk &lt;-<span class="st"> </span><span class="kw">diag</span>(svdX<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k])</span>
<span><a rel="noopener" href="#cb3-5"></a>Zk &lt;-<span class="st"> </span>Uk<span class="op">%*%</span>Dk</span>
<span><a rel="noopener" href="#cb3-6"></a><span class="kw">colnames</span>(Zk) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;Z&quot;</span>,<span class="dv">1</span><span class="op">:</span>k)</span>
<span><a rel="noopener" href="#cb3-7"></a></span>
<span><a rel="noopener" href="#cb3-8"></a></span>
<span><a rel="noopener" href="#cb3-9"></a>Zk <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb3-10"></a><span class="st">  </span>as.data.frame <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb3-11"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Reject_Status =</span> reject_status <span class="op">%&gt;%</span><span class="st"> </span>as.factor) <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb3-12"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span> Z1, <span class="dt">y =</span> Z2, <span class="dt">color =</span> Reject_Status)) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb3-13"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>On the scatterplot each dot represents a patient and the colour is the rejection status of that patient, they are plotted against the first PC (x-axis) and the second PC (Y-axis). We see there is much overlap between the two groups, but the second PC might be interesting with regards to the rejection status, as more red dots can be seen in the upper area of the plot and more blue dots in the lower area.</p>
<div class="section level3">
<h3>Asses loadings</h3>
<p>Because a biplot in this setting will not be interpretable, we only assess the loadings of the first two PC’s, for each gene. These loadings represent the contribution of each gene to the PC.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb4-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span><a rel="noopener" href="#cb4-2"></a></span>
<span><a rel="noopener" href="#cb4-3"></a><span class="kw">hist</span>(Vk[, <span class="dv">1</span>], <span class="dt">breaks =</span> <span class="dv">50</span>, <span class="dt">xlab =</span> <span class="st">&quot;PC 1 loadings&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span><a rel="noopener" href="#cb4-4"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">c</span>(</span>
<span><a rel="noopener" href="#cb4-5"></a>  <span class="kw">quantile</span>(Vk[, <span class="dv">1</span>], <span class="fl">0.05</span>),</span>
<span><a rel="noopener" href="#cb4-6"></a>  <span class="kw">quantile</span>(Vk[, <span class="dv">1</span>], <span class="fl">0.95</span>)), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span><a rel="noopener" href="#cb4-7"></a></span>
<span><a rel="noopener" href="#cb4-8"></a></span>
<span><a rel="noopener" href="#cb4-9"></a><span class="kw">hist</span>(Vk[, <span class="dv">2</span>], <span class="dt">breaks =</span> <span class="dv">50</span>, <span class="dt">xlab =</span> <span class="st">&quot;PC 2 loadings&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span><a rel="noopener" href="#cb4-10"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">c</span>(</span>
<span><a rel="noopener" href="#cb4-11"></a>  <span class="kw">quantile</span>(Vk[, <span class="dv">2</span>], <span class="fl">0.05</span>),</span>
<span><a rel="noopener" href="#cb4-12"></a>  <span class="kw">quantile</span>(Vk[, <span class="dv">2</span>], <span class="fl">0.95</span>)</span>
<span><a rel="noopener" href="#cb4-13"></a>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>Many genes contribute to the first PC’s, the vast majority (95%) of these genes are within the two vertical red lines. We can see no distinct outliers, no genes in particular are really driving these PC’s. The loadings of the first PC are skewed to the right, the do not follow a normal distribution. There a very few negative loadings compared to positive, meaning most genes will positively influence the first PC when their gene expression is upregulated.</p>
</div>
</div>
<div class="section level2">
<h2>LDA</h2>
<p>A better way to visualize a potential difference between the two rejection statusses is with Fisher’s Linear Discriminant Analysis. We will look for a direction <span class="math inline">\(a\)</span> in the 13699-dimensional space, so that the orthogonal projections of the predictors (<span class="math inline">\(X^Ta\)</span>) show a maximized ratio between the SSB (between sum of squares) and the SSE (within sum of squares).</p>
<p><span class="math display">\[
V = ArgMax_a\frac{a^TBa}{a^TWa}
\]</span></p>
<p><span class="math inline">\(B\)</span> is the between covariance matrix of X, <span class="math inline">\(W\)</span> is the within covariance matrix of X, and <span class="math inline">\(a^TWa = 1\)</span> in order to have a unique solution</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb5-1"></a>kid_lda &lt;-<span class="st"> </span><span class="kw">lda</span>(<span class="dt">x =</span> X, <span class="dt">grouping =</span> reject_status)</span></code></pre></div>
<pre><code>## Warning in lda.default(x, grouping, ...): variables are collinear</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb7-1"></a>cols &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;n&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;t&quot;</span> =<span class="st"> &quot;blue&quot;</span>)</span>
<span><a rel="noopener" href="#cb7-2"></a>Vlda &lt;-<span class="st"> </span>kid_lda<span class="op">$</span>scaling</span>
<span><a rel="noopener" href="#cb7-3"></a>Zlda &lt;-<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>Vlda</span>
<span><a rel="noopener" href="#cb7-4"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span><a rel="noopener" href="#cb7-5"></a><span class="kw">boxplot</span>(Zlda <span class="op">~</span><span class="st"> </span>reject_status, <span class="dt">col =</span> cols, <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="st">&quot;Z&quot;</span>[<span class="dv">1</span>]), </span>
<span><a rel="noopener" href="#cb7-6"></a>        <span class="dt">main =</span> <span class="st">&quot;Separation of non rejected and rejected kidneys by LDA&quot;</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>With LDA we see a clear distinction between the two groups, with a little overlap.</p>
<p>This shows that we might assume that there is a difference in gene expression between patients who show no rejection of the kidney compared to those whose transplanted kidney is rejected.</p>
</div>
</div>
<div class="section level1">
<h1>Hypothesis testing</h1>
<p>We will test the following hypothesis</p>
<p><span class="math inline">\(H_{0i}: \mu_{NRi} = \mu_{Ri}\)</span></p>
<p>against the alternative</p>
<p><span class="math inline">\(H_{1i}: \mu_{NRi} \neq \mu_{Ri}\)</span></p>
<p>for all 13669 genes.</p>
<p>This will cause a big multiple testing problem, which we will then correct using the Benjamini and Hochberg method with the False Discover Rate (FDR) set at 10%.</p>
<div class="section level2">
<h2>Two-sided two-sample t-test</h2>
<p>A two-sided two-sample t-test is executed for all 13669 genes. We have unequal sample sizes <span class="math inline">\(n_0\)</span> and <span class="math inline">\(n_1\)</span> but assume equal variance.</p>
<p><span class="math inline">\(t=\frac{\overline{X_0}-\overline{X_1}}{S\sqrt{\frac{1}{n_0}+\frac{1}{n_1}}}\)</span></p>
<p>with <span class="math inline">\(n_0=174\)</span> and <span class="math inline">\(n_1 = 76\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb8-1"></a>ttest_results &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X, <span class="dv">2</span>, <span class="cf">function</span>(x) {</span>
<span><a rel="noopener" href="#cb8-2"></a>  t_test &lt;-<span class="st"> </span><span class="kw">t.test</span>(x <span class="op">~</span><span class="st"> </span>reject_status)</span>
<span><a rel="noopener" href="#cb8-3"></a>  p_val &lt;-<span class="st"> </span>t_test<span class="op">$</span>p.value</span>
<span><a rel="noopener" href="#cb8-4"></a>  stat &lt;-<span class="st"> </span>t_test<span class="op">$</span>statistic</span>
<span><a rel="noopener" href="#cb8-5"></a>  df &lt;-<span class="st"> </span>t_test<span class="op">$</span>parameter</span>
<span><a rel="noopener" href="#cb8-6"></a>  <span class="co">## Return values in named vector</span></span>
<span><a rel="noopener" href="#cb8-7"></a>  <span class="kw">c</span>(stat, <span class="st">&quot;p_val&quot;</span> =<span class="st"> </span>p_val, df)</span>
<span><a rel="noopener" href="#cb8-8"></a>}))</span>
<span><a rel="noopener" href="#cb8-9"></a></span>
<span><a rel="noopener" href="#cb8-10"></a><span class="kw">head</span>(ttest_results)</span></code></pre></div>
<pre><code>##                      t        p_val       df
## X121_at      4.4851733 1.274777e-05 185.4869
## X1255_g_at   0.3897885 6.972426e-01 150.8980
## X1294_at    -5.7075309 8.062938e-08 123.2678
## X1405_i_at  -8.7570018 5.921187e-15 139.8258
## X1431_at     0.2373303 8.127265e-01 149.0206
## X1552261_at  0.7896813 4.310042e-01 144.9068</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb10-1"></a>p_vals &lt;-<span class="st"> </span>ttest_results[, <span class="st">&quot;p_val&quot;</span>]</span>
<span><a rel="noopener" href="#cb10-2"></a><span class="kw">hist</span>(</span>
<span><a rel="noopener" href="#cb10-3"></a>  p_vals,</span>
<span><a rel="noopener" href="#cb10-4"></a>  <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">0.05</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p-value&quot;</span>,</span>
<span><a rel="noopener" href="#cb10-5"></a>  <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">5000</span>)</span>
<span><a rel="noopener" href="#cb10-6"></a>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>This plot shows us that a large proportion of p-values falls under the significance threshold of 0.05.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb11-1"></a>alpha &lt;-<span class="st"> </span><span class="fl">0.05</span></span>
<span><a rel="noopener" href="#cb11-2"></a><span class="kw">sum</span>(p_vals <span class="op">&lt;</span><span class="st"> </span>alpha)</span></code></pre></div>
<pre><code>## [1] 4012</code></pre>
<p>This gives us 4012 significant results. Because we perform 13669 simultaneous t-tests at a 0.05 significance level, 0.05 x 13699 = 684.95 results would come up positive if all nullhypotheses were true, meaning we expect 685 false positives to occur if we don’t correct for multiple testing.</p>
</div>
<div class="section level2">
<h2>BH95</h2>
<p>The next step is to correct the multiple testing problem with an FDR set at 10%, using the Benjamini and Hochberg (1995) method. An FDR of 10% means we will tolerate on average 10% false postives among all the positive outcomes:</p>
<p><span class="math inline">\(FDR = E[\frac{FP}{R}] = E[FDP]\)</span></p>
<p>The BH95 method consists of calculating adjusted p-values, which we then test against our FDR controlled at <span class="math inline">\(\alpha = 0,1\)</span>:</p>
<p><span class="math inline">\(q_{(i)}=min[min_{j=i, ...,m}(\frac{mp_{(j)}}{j}), 1]\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb13-1"></a>fdr &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(p_vals, <span class="dt">method =</span> <span class="st">&quot;BH&quot;</span>)</span>
<span><a rel="noopener" href="#cb13-2"></a></span>
<span><a rel="noopener" href="#cb13-3"></a><span class="kw">plot</span>(</span>
<span><a rel="noopener" href="#cb13-4"></a>  p_vals[<span class="kw">order</span>(p_vals)], fdr[<span class="kw">order</span>(p_vals)],</span>
<span><a rel="noopener" href="#cb13-5"></a>  <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">cex =</span> <span class="fl">0.6</span>, <span class="dt">xlab =</span> <span class="st">&quot;p-value&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;FDR-adjusted p-value&quot;</span>, <span class="dt">col =</span> <span class="dv">4</span></span>
<span><a rel="noopener" href="#cb13-6"></a>)</span>
<span><a rel="noopener" href="#cb13-7"></a><span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb14-1"></a><span class="kw">sum</span>(fdr <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.10</span>)</span></code></pre></div>
<pre><code>## [1] 3106</code></pre>
<p>When the FDR is controlled at 10%, we still find 3106 significant discoveries. We have to take into account that we are testing a subset of 25% of the most variable genes from the original data set. This explains why such a large proportion of genes comes back as possibly having differential expressions.</p>
<p>We can further explore the distribution of these adjusted p-values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb16-1"></a>fdr_df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fdr)</span>
<span><a rel="noopener" href="#cb16-2"></a>fdr_df<span class="op">$</span>ID &lt;-<span class="st"> </span><span class="kw">rownames</span>(fdr_df)</span>
<span><a rel="noopener" href="#cb16-3"></a></span>
<span><a rel="noopener" href="#cb16-4"></a>sign_genes &lt;-<span class="st"> </span>fdr_df <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb16-5"></a><span class="st">  </span><span class="kw">filter</span>(fdr_df<span class="op">$</span>fdr <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.10</span>)</span>
<span><a rel="noopener" href="#cb16-6"></a>ord_genes &lt;-<span class="st"> </span>sign_genes[<span class="kw">order</span>(sign_genes<span class="op">$</span>fdr),, drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span><a rel="noopener" href="#cb16-7"></a></span>
<span><a rel="noopener" href="#cb16-8"></a><span class="kw">head</span>(ord_genes, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##                       fdr           ID
## X226474_at   2.097251e-17   X226474_at
## X200904_at   2.786089e-17   X200904_at
## X204205_at   4.307637e-17   X204205_at
## X231577_s_at 4.307637e-17 X231577_s_at
## X202748_at   4.932657e-17   X202748_at
## X204279_at   4.932657e-17   X204279_at
## X202270_at   6.001809e-17   X202270_at
## X203915_at   8.514508e-17   X203915_at
## X209312_x_at 1.279802e-16 X209312_x_at
## X205890_s_at 1.615840e-16 X205890_s_at</code></pre>
<p>We created a list with the significant p-values sorted from small to large (here we only show 10 genes with the lowest p-values).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb18-1"></a>ord_genes<span class="op">$</span>logp &lt;-<span class="st"> </span><span class="kw">log</span>(ord_genes<span class="op">$</span>fdr)</span>
<span><a rel="noopener" href="#cb18-2"></a><span class="kw">hist</span>(ord_genes<span class="op">$</span>logp,</span>
<span><a rel="noopener" href="#cb18-3"></a>     <span class="dt">breaks =</span> <span class="dv">50</span>,</span>
<span><a rel="noopener" href="#cb18-4"></a>     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">800</span>),</span>
<span><a rel="noopener" href="#cb18-5"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">40</span>, <span class="dv">5</span>),</span>
<span><a rel="noopener" href="#cb18-6"></a>     <span class="dt">main =</span> <span class="st">&quot;Histogram of the logtransformed significant adjusted p-values&quot;</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>In order to see how these p-values are distributed, we logtransformed them and plotted these values on a histogram. This shows us that there are some very small adjusted p-values. The genes corresponding to these p-values might be interesting to look at.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb19-1"></a>under10 &lt;-<span class="st"> </span>ord_genes <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb19-2"></a><span class="st">  </span><span class="kw">filter</span>(ord_genes<span class="op">$</span>logp <span class="op">&lt;</span><span class="st"> </span><span class="dv">-10</span>)</span>
<span><a rel="noopener" href="#cb19-3"></a><span class="kw">nrow</span>(under10)</span></code></pre></div>
<pre><code>## [1] 731</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb21-1"></a>under30 &lt;-<span class="st"> </span>ord_genes <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb21-2"></a><span class="st">  </span><span class="kw">filter</span>(ord_genes<span class="op">$</span>logp <span class="op">&lt;</span><span class="st"> </span><span class="dv">-30</span>)</span>
<span><a rel="noopener" href="#cb21-3"></a><span class="kw">nrow</span>(under30)</span></code></pre></div>
<pre><code>## [1] 66</code></pre>
</div>
<div class="section level2">
<h2>Local fdr</h2>
<p>Another way to explore potential differential expression is by using the local fdr method. This gives us the probability that a gene is a null (gene for which <span class="math inline">\(H_{0i}\)</span> is true, i.e.&#160;no differential expression) given a certain gene: <span class="math inline">\(fdr(z) =P[null|z]\)</span>.</p>
<p>If the local fdr is sufficiently small for a given gene, it will be very probable that this gene will be a true positive.</p>
<p>For this method we need to transform the t-statistics into z-scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb23-1"></a>t &lt;-<span class="st"> </span>ttest_results[, <span class="st">&quot;t&quot;</span>]</span>
<span><a rel="noopener" href="#cb23-2"></a><span class="kw">length</span>(t)</span></code></pre></div>
<pre><code>## [1] 13669</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb25-1"></a>z1 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(t))</span>
<span><a rel="noopener" href="#cb25-2"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="kw">length</span>(t)){</span>
<span><a rel="noopener" href="#cb25-3"></a>  z1[i] &lt;-<span class="st"> </span><span class="kw">qnorm</span>(<span class="kw">pt</span>(t[i], <span class="dt">df=</span> <span class="dv">248</span>))</span>
<span><a rel="noopener" href="#cb25-4"></a>}</span>
<span><a rel="noopener" href="#cb25-5"></a><span class="kw">mean</span>(z1)  </span></code></pre></div>
<pre><code>## [1] -0.9261074</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb27-1"></a><span class="kw">sd</span>(z1)</span></code></pre></div>
<pre><code>## [1] 2.112927</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb29-1"></a>lfdr &lt;-<span class="st"> </span><span class="kw">locfdr</span>(z1, <span class="dt">plot =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>This graph shows us that we can expect non-nulls for extreme negative z-values, and (almost) none for positive z-values.</p>
<p>The expected false discovery rate <span class="math inline">\(Efdr = E_{f1}[fdr(z)]\)</span> is the expected probability of falsely finding a null als a significant result and is a measure for the power of the tests. We want this to be as small as possible. Here the Efdr is 0.235, we have a 23.5% chance of falsely claiming a significant result.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb30-1"></a>lfdr1 &lt;-<span class="st"> </span>lfdr<span class="op">$</span>fdr</span>
<span><a rel="noopener" href="#cb30-2"></a>gene_ID &lt;-<span class="st"> </span><span class="kw">colnames</span>(X)</span>
<span><a rel="noopener" href="#cb30-3"></a>lfdr_df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(gene_ID, lfdr1))</span>
<span><a rel="noopener" href="#cb30-4"></a>lfdr_df<span class="op">$</span>lfdr1 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>((lfdr_df<span class="op">$</span>lfdr1))</span>
<span><a rel="noopener" href="#cb30-5"></a>sig &lt;-<span class="st"> </span>lfdr_df[fdr_df<span class="op">$</span>ID <span class="op">%in%</span><span class="st"> </span>sign_genes<span class="op">$</span>ID,]</span>
<span><a rel="noopener" href="#cb30-6"></a><span class="kw">hist</span>(sig<span class="op">$</span>lfdr1)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<p>When we now only look at the genes that were returned significant with FDR = 10%, we see that a large proportion of these genes score poorly on a local fdr basis (many are around 100% local fdr), but we also see many genes under 10% local fdr.</p>
<p>We could filter additionally with local fdr set at 10%</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb31-1"></a>Sig_sig &lt;-<span class="st"> </span>sig <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb31-2"></a><span class="st">  </span><span class="kw">filter</span>(sig<span class="op">$</span>lfdr1 <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.10</span>)</span>
<span><a rel="noopener" href="#cb31-3"></a><span class="kw">nrow</span>(Sig_sig)</span></code></pre></div>
<pre><code>## [1] 522</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb33-1"></a>most_sig_p &lt;-<span class="st"> </span>sign_genes[sign_genes<span class="op">$</span>ID <span class="op">%in%</span><span class="st"> </span>Sig_sig<span class="op">$</span>gene_ID,]</span>
<span><a rel="noopener" href="#cb33-2"></a>most_sig_p<span class="op">$</span>lfdr &lt;-<span class="st"> </span>Sig_sig<span class="op">$</span>lfdr1</span>
<span><a rel="noopener" href="#cb33-3"></a><span class="kw">colnames</span>(most_sig_p) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;adj_p&quot;</span>, <span class="st">&quot;ID&quot;</span>, <span class="st">&quot;lfdr&quot;</span>)</span>
<span><a rel="noopener" href="#cb33-4"></a><span class="kw">rownames</span>(most_sig_p) &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span><a rel="noopener" href="#cb33-5"></a><span class="kw">head</span>(most_sig_p)</span></code></pre></div>
<pre><code>##          adj_p            ID         lfdr
## 1 7.425386e-13    X1405_i_at 0.0003620658
## 2 1.149318e-11 X1552701_a_at 0.0011650331
## 3 4.415244e-12 X1552703_s_at 0.0008936532
## 4 2.077969e-12 X1553906_s_at 0.0005719656
## 5 1.094182e-11 X1554240_a_at 0.0007464837
## 6 1.195978e-11 X1554899_s_at 0.0014140880</code></pre>
<p>This gives us 522 genes that have at the most a probability of 10% of being a null (no differential expression). The fill list can be seen in the appendix.</p>
</div>
</div>
<div class="section level1">
<h1>Prediction</h1>
<p>For the third research question we will see if we can find a prediction model that can predict kidney rejection status from gene expression. The problem we encounter when dealing with high dimensional data (<span class="math inline">\(p&gt;n\)</span>) is that we have too many variables, which will cause the model to be grossly overfitted and useless for prediction purposes.</p>
<p>3 methods can be used to circumvent this problem:</p>
<ul>
<li><p>Principal Component Regression (PCR)</p></li>
<li><p>Penalized Regression: Ridge</p></li>
<li><p>Penalized Regression: Lasso</p></li>
</ul>
<p>Before we begin applying these methods we divide the original data set in training data and test data. This way the training data is used for building the model, while we keep the test data to evaluate our final model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb35-1"></a><span class="kw">set.seed</span>(<span class="dv">2021</span>)</span>
<span><a rel="noopener" href="#cb35-2"></a><span class="co">####</span></span>
<span><a rel="noopener" href="#cb35-3"></a>Y &lt;-<span class="st"> </span>kidney_data<span class="op">$</span>Reject_Status</span>
<span><a rel="noopener" href="#cb35-4"></a>n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span><a rel="noopener" href="#cb35-5"></a>nTrain &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.7</span><span class="op">*</span>n)</span>
<span><a rel="noopener" href="#cb35-6"></a>indTrain &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n,nTrain)</span>
<span><a rel="noopener" href="#cb35-7"></a>XTrain &lt;-<span class="st"> </span>X[indTrain,]</span>
<span><a rel="noopener" href="#cb35-8"></a>YTrain &lt;-<span class="st"> </span>Y[indTrain]</span>
<span><a rel="noopener" href="#cb35-9"></a>XTest &lt;-<span class="st"> </span>X[<span class="op">-</span>indTrain,]</span>
<span><a rel="noopener" href="#cb35-10"></a>YTest &lt;-<span class="st"> </span>Y[<span class="op">-</span>indTrain]</span>
<span><a rel="noopener" href="#cb35-11"></a><span class="kw">table</span>(YTest)</span></code></pre></div>
<pre><code>## YTest
##  0  1 
## 52 23</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb37-1"></a><span class="kw">table</span>(YTrain)</span></code></pre></div>
<pre><code>## YTrain
##   0   1 
## 122  53</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb39-1"></a><span class="kw">table</span>(Y)</span></code></pre></div>
<pre><code>## Y
##   0   1 
## 174  76</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb41-1"></a><span class="dv">23</span><span class="op">/</span>(<span class="dv">52</span><span class="op">+</span><span class="dv">23</span>) </span></code></pre></div>
<pre><code>## [1] 0.3066667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb43-1"></a><span class="dv">53</span><span class="op">/</span>(<span class="dv">122</span><span class="op">+</span><span class="dv">53</span>)</span></code></pre></div>
<pre><code>## [1] 0.3028571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb45-1"></a><span class="dv">76</span><span class="op">/</span><span class="dv">250</span></span></code></pre></div>
<pre><code>## [1] 0.304</code></pre>
<p>Our randomly chosen test data set has 30.6% rejection cases on a total of 75 cases, training data 30.3% on 175 cases and our original data had 30.4%. This means the same ratio is roughly preserved.</p>
<div class="section level2">
<h2>PCR</h2>
<p>(PCR code is set to eval = FALSE because it was compiled separately and is not compatible with the rest of the code and impossible to adapt it in time. The output can be found in the appendix)</p>
<p>We will reduce our original matrix of predictors <span class="math inline">\(X\)</span> of <span class="math inline">\(p\)</span> dimensions to a new matrix <span class="math inline">\(Z\)</span> of <span class="math inline">\(n\)</span> dimensions. From this new matrix we will only select those PC’s that contribute the most with regards to our research question. These are not necessarily the first PC’s.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb47-1"></a><span class="co">## Calculate PCA and extract scores</span></span>
<span><a rel="noopener" href="#cb47-2"></a>pca_X &lt;-<span class="st"> </span><span class="kw">prcomp</span>(XTrain)</span>
<span><a rel="noopener" href="#cb47-3"></a>pca_var &lt;-<span class="st"> </span>pca_X<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></span>
<span><a rel="noopener" href="#cb47-4"></a>pca_var_per &lt;-<span class="st"> </span><span class="kw">round</span>(pca_var<span class="op">/</span><span class="kw">sum</span>(pca_var)<span class="op">*</span><span class="dv">100</span>, <span class="dv">1</span>)</span>
<span><a rel="noopener" href="#cb47-5"></a><span class="kw">barplot</span>(pca_var)</span>
<span><a rel="noopener" href="#cb47-6"></a>Z &lt;-<span class="st"> </span>pca_X<span class="op">$</span>x</span>
<span><a rel="noopener" href="#cb47-7"></a></span>
<span><a rel="noopener" href="#cb47-8"></a><span class="co">## Total number of available PCs</span></span>
<span><a rel="noopener" href="#cb47-9"></a>n_PC &lt;-<span class="st"> </span><span class="kw">ncol</span>(Z)</span>
<span><a rel="noopener" href="#cb47-10"></a></span>
<span><a rel="noopener" href="#cb47-11"></a><span class="co">## cv.glm() requires the response and predictors in one data.frame, so we need</span></span>
<span><a rel="noopener" href="#cb47-12"></a><span class="co">## to combine them back together</span></span>
<span><a rel="noopener" href="#cb47-13"></a>fit_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(YTrain, Z)</span>
<span><a rel="noopener" href="#cb47-14"></a><span class="kw">head</span>(fit_data)</span>
<span><a rel="noopener" href="#cb47-15"></a></span>
<span><a rel="noopener" href="#cb47-16"></a><span class="co">## Example of PC Log. Reg. with all PCs</span></span>
<span><a rel="noopener" href="#cb47-17"></a>full_model &lt;-<span class="st"> </span><span class="kw">glm</span>(YTrain <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> fit_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span><a rel="noopener" href="#cb47-18"></a><span class="co"># summary(full_model)</span></span></code></pre></div>
<div class="section level3">
<h3>Cross-validation</h3>
<p>In order evaluate our model building process by estimating the Expected Test Error: <span class="math inline">\(E_\tau[Err_\tau]=E_{Y^*, X^*, \tau}[(\hat{m}(X^*)-Y^*)^2]\)</span>. We want this expected test error to be as small as possible.</p>
<p>The ETE is estimated using a 4 fold cross-validation method: <span class="math inline">\(CV_k = \frac{1}{k}\sum_{j=1}^k\frac{1}{n_j}\sum_{i\epsilon S_j)}(Y_i-\hat{m}^{-S_j}(x_i))^2\)</span> with <span class="math inline">\(k=4\)</span></p>
<p>This method will be used to select our principal components (and later to estimate the optimal <span class="math inline">\(\lambda\)</span> for the ridge and lasso penalized regression).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb48-1"></a><span class="co">## 4-fold Cross-validation on this one particular model, using AUC and K = 4</span></span>
<span><a rel="noopener" href="#cb48-2"></a>full_model_cv &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(</span>
<span><a rel="noopener" href="#cb48-3"></a>  <span class="dt">data =</span> fit_data,  <span class="dt">glmfit =</span> full_model,</span>
<span><a rel="noopener" href="#cb48-4"></a>  <span class="dt">cost =</span> pROC<span class="op">::</span>auc, <span class="dt">K =</span> <span class="dv">4</span>  <span class="co"># note: specify the auc function (from pROC) without`()`!</span></span>
<span><a rel="noopener" href="#cb48-5"></a>)</span>
<span><a rel="noopener" href="#cb48-6"></a></span>
<span><a rel="noopener" href="#cb48-7"></a><span class="co">## We&#39;ll just use the raw one here</span></span>
<span><a rel="noopener" href="#cb48-8"></a>full_model_cv<span class="op">$</span>delta[<span class="dv">1</span>] <span class="co"># This is the AUC for this particular model estimated by AUC</span></span>
<span><a rel="noopener" href="#cb48-9"></a></span>
<span><a rel="noopener" href="#cb48-10"></a><span class="co">## wrap this code in a for-loop and repeat for each number of PCs</span></span>
<span><a rel="noopener" href="#cb48-11"></a>cv_auc &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, n_PC)</span>
<span><a rel="noopener" href="#cb48-12"></a><span class="kw">set.seed</span>(<span class="dv">12</span>) <span class="co"># seed for reproducibility</span></span>
<span><a rel="noopener" href="#cb48-13"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n_PC)) {</span>
<span><a rel="noopener" href="#cb48-14"></a>  <span class="co">## Prepare fit_data; subset number of PCs to i</span></span>
<span><a rel="noopener" href="#cb48-15"></a>  fit_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(YTrain, Z[, <span class="dv">1</span><span class="op">:</span>i, <span class="dt">drop =</span> <span class="ot">FALSE</span>])  <span class="co"># use drop = FALSE to avoid problems when subsetting single column</span></span>
<span><a rel="noopener" href="#cb48-16"></a>  pcr_mod &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(</span>
<span><a rel="noopener" href="#cb48-17"></a>    <span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> fit_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span><a rel="noopener" href="#cb48-18"></a>  )</span>
<span><a rel="noopener" href="#cb48-19"></a>  </span>
<span><a rel="noopener" href="#cb48-20"></a> <span class="co">## Do 4-fold CV while suppressing Warnings and Messages </span></span>
<span><a rel="noopener" href="#cb48-21"></a>  cv &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(</span>
<span><a rel="noopener" href="#cb48-22"></a>    <span class="kw">suppressMessages</span>(</span>
<span><a rel="noopener" href="#cb48-23"></a>      <span class="kw">cv.glm</span>(fit_data, pcr_mod, <span class="dt">cost =</span> pROC<span class="op">::</span>auc, <span class="dt">K =</span> <span class="dv">4</span>)</span>
<span><a rel="noopener" href="#cb48-24"></a>    )</span>
<span><a rel="noopener" href="#cb48-25"></a>  )</span>
<span><a rel="noopener" href="#cb48-26"></a>  cv_auc[i] &lt;-<span class="st"> </span>cv<span class="op">$</span>delta[<span class="dv">1</span>]</span>
<span><a rel="noopener" href="#cb48-27"></a>}</span>
<span><a rel="noopener" href="#cb48-28"></a><span class="kw">names</span>(cv_auc) &lt;-<span class="st"> </span><span class="kw">seq_along</span>(cv_auc)</span>
<span><a rel="noopener" href="#cb48-29"></a>cv_auc</span>
<span><a rel="noopener" href="#cb48-30"></a><span class="co">## Finding the optimal nr. of PCs corresponds to finding the max. AUC</span></span>
<span><a rel="noopener" href="#cb48-31"></a>optim_nPC &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which.max</span>(cv_auc))</span>
<span><a rel="noopener" href="#cb48-32"></a>optim_nPC</span>
<span><a rel="noopener" href="#cb48-33"></a></span>
<span><a rel="noopener" href="#cb48-34"></a><span class="kw">plot</span>(<span class="kw">names</span>(cv_auc), cv_auc, <span class="dt">xlab =</span> <span class="st">&quot;n PCs&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;AUC&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span><a rel="noopener" href="#cb48-35"></a><span class="kw">abline</span>(<span class="dt">v =</span> optim_nPC, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3>Model evaluation</h3>
<p>We evaluate the model by calculating the Area Under the Curve (AUC), specifically for the Receiver Operating Characteristic (ROC) curve. This curve maps the threshold c, which is the critical value which separates the negative outcomes from the positive and which decides the sensitivity (P(true positive)) and the specificity (P(ctrue negative)) of a test. This c is plotted against the sensitivity on the y-axis and 1-specificity (P(false positive)) on the x-axis. The larger the area under this curve means, the better the prediction value of a model (a perfect test which has no false positive or false negative outcomes has a AUC of 1).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb49-1"></a><span class="co">## prediction using Test data</span></span>
<span><a rel="noopener" href="#cb49-2"></a>pca_X &lt;-<span class="st"> </span><span class="kw">prcomp</span>(XTrain)</span>
<span><a rel="noopener" href="#cb49-3"></a><span class="co"># YTrain &lt;- as.factor(YTrain)</span></span>
<span><a rel="noopener" href="#cb49-4"></a><span class="co"># dim(Xtrain)</span></span>
<span><a rel="noopener" href="#cb49-5"></a>Z &lt;-<span class="st"> </span>pca_X<span class="op">$</span>x</span>
<span><a rel="noopener" href="#cb49-6"></a><span class="kw">dim</span>(Z)</span>
<span><a rel="noopener" href="#cb49-7"></a>pca_var &lt;-<span class="st"> </span>pca_X<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></span>
<span><a rel="noopener" href="#cb49-8"></a>pca_var_per &lt;-<span class="st"> </span><span class="kw">round</span>(pca_var<span class="op">/</span><span class="kw">sum</span>(pca_var)<span class="op">*</span><span class="dv">100</span>, <span class="dv">1</span>)</span>
<span><a rel="noopener" href="#cb49-9"></a><span class="kw">barplot</span>(pca_var)</span>
<span><a rel="noopener" href="#cb49-10"></a></span>
<span><a rel="noopener" href="#cb49-11"></a>opt_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(YTrain, Z[, <span class="dv">1</span><span class="op">:</span><span class="dv">13</span>])</span>
<span><a rel="noopener" href="#cb49-12"></a><span class="kw">head</span>(opt_data)</span>
<span><a rel="noopener" href="#cb49-13"></a><span class="kw">table</span>(YTrain)</span>
<span><a rel="noopener" href="#cb49-14"></a><span class="kw">summary</span>(YTrain)</span>
<span><a rel="noopener" href="#cb49-15"></a></span>
<span><a rel="noopener" href="#cb49-16"></a>opt_model &lt;-<span class="st"> </span><span class="kw">glm</span>(YTrain <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> opt_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-17"></a><span class="kw">summary</span>(opt_model)</span>
<span><a rel="noopener" href="#cb49-18"></a><span class="co">#The estimators of coefficients that have been obtained (βZ), as stated in the introduction can be multiplied by matrix V to obtain βX.</span></span>
<span><a rel="noopener" href="#cb49-19"></a>beta.Z &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(opt_model<span class="op">$</span>coefficients[<span class="dv">2</span><span class="op">:</span><span class="dv">14</span>])</span>
<span><a rel="noopener" href="#cb49-20"></a>V &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(pca_X<span class="op">$</span>rotation[,<span class="dv">2</span><span class="op">:</span><span class="dv">14</span>])</span>
<span><a rel="noopener" href="#cb49-21"></a></span>
<span><a rel="noopener" href="#cb49-22"></a><span class="co"># In order to compare the prediction, I am predicting the values based on the βX coefficient estimates calculated before, according to this equation</span></span>
<span><a rel="noopener" href="#cb49-23"></a><span class="co"># βX = V X βZ</span></span>
<span><a rel="noopener" href="#cb49-24"></a>beta.X &lt;-<span class="st"> </span>V <span class="op">%*%</span><span class="st"> </span>beta.Z</span>
<span><a rel="noopener" href="#cb49-25"></a><span class="kw">head</span>(beta.X)</span>
<span><a rel="noopener" href="#cb49-26"></a></span>
<span><a rel="noopener" href="#cb49-27"></a>pred.test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(XTest)</span>
<span><a rel="noopener" href="#cb49-28"></a><span class="co"># head(pred.test)</span></span>
<span><a rel="noopener" href="#cb49-29"></a><span class="co"># head(beta.X)</span></span>
<span><a rel="noopener" href="#cb49-30"></a>y.pred.test2 &lt;-<span class="st"> </span>pred.test <span class="op">%*%</span><span class="st"> </span>beta.X</span>
<span><a rel="noopener" href="#cb49-31"></a></span>
<span><a rel="noopener" href="#cb49-32"></a><span class="kw">plot</span>(y.pred.test2)</span>
<span><a rel="noopener" href="#cb49-33"></a></span>
<span><a rel="noopener" href="#cb49-34"></a>logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(logit){</span>
<span><a rel="noopener" href="#cb49-35"></a>  odds &lt;-<span class="st"> </span><span class="kw">exp</span>(logit)</span>
<span><a rel="noopener" href="#cb49-36"></a>  prob &lt;-<span class="st"> </span>odds <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>odds)</span>
<span><a rel="noopener" href="#cb49-37"></a>  <span class="kw">return</span>(prob)</span>
<span><a rel="noopener" href="#cb49-38"></a>}</span>
<span><a rel="noopener" href="#cb49-39"></a></span>
<span><a rel="noopener" href="#cb49-40"></a>prob &lt;-<span class="st"> </span><span class="kw">logit2prob</span>(y.pred.test2)</span>
<span><a rel="noopener" href="#cb49-41"></a>XTest<span class="op">$</span>predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(prob <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="st">&quot;1&quot;</span>, <span class="st">&quot;0&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-42"></a><span class="kw">table</span>(YTest)</span>
<span><a rel="noopener" href="#cb49-43"></a></span>
<span><a rel="noopener" href="#cb49-44"></a>confusion_mat &lt;-<span class="st"> </span><span class="kw">table</span>(YTest, XTest<span class="op">$</span>predict)</span>
<span><a rel="noopener" href="#cb49-45"></a><span class="kw">rownames</span>(confusion_mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;obs.0&quot;</span>, <span class="st">&quot;obs.1&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-46"></a><span class="kw">colnames</span>(confusion_mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;pred.0&quot;</span>, <span class="st">&quot;pred.1&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-47"></a>confusion_mat</span>
<span><a rel="noopener" href="#cb49-48"></a><span class="kw">dim</span>(<span class="kw">data.frame</span>(prob))</span>
<span><a rel="noopener" href="#cb49-49"></a></span>
<span><a rel="noopener" href="#cb49-50"></a>pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob, YTest)</span>
<span><a rel="noopener" href="#cb49-51"></a></span>
<span><a rel="noopener" href="#cb49-52"></a>perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-53"></a><span class="kw">plot</span>(perf)</span>
<span><a rel="noopener" href="#cb49-54"></a>auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span><a rel="noopener" href="#cb49-55"></a>auc &lt;-<span class="st"> </span>auc<span class="op">@</span>y.values[[<span class="dv">1</span>]]</span>
<span><a rel="noopener" href="#cb49-56"></a><span class="kw">paste</span>(<span class="st">&quot;AUC:&quot;</span>, auc)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2>Ridge</h2>
<p>The penalized Ridge regression method consists of adding a penalty term to the usual sum of least squares solution:</p>
<p><span class="math inline">\(SSE_{pen}=||Y-X\beta||^2 + \lambda||\beta||_2^2\)</span></p>
<p>where <span class="math inline">\(||\beta||_2^2 = \sum_{j=1}^p\beta_j^2\)</span> is the <span class="math inline">\(L_2\)</span> penalty term and <span class="math inline">\(\lambda &gt;0\)</span> is the penalty parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb50-1"></a>ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span><a rel="noopener" href="#cb50-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span><a rel="noopener" href="#cb50-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span><a rel="noopener" href="#cb50-4"></a>  <span class="dt">alpha =</span> <span class="dv">0</span>,         <span class="co"># ridge: alpha = 0</span></span>
<span><a rel="noopener" href="#cb50-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)  </span>
<span><a rel="noopener" href="#cb50-6"></a></span>
<span><a rel="noopener" href="#cb50-7"></a><span class="kw">plot</span>(ridge, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb51-1"></a>cv_ridge &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(</span>
<span><a rel="noopener" href="#cb51-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span><a rel="noopener" href="#cb51-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span><a rel="noopener" href="#cb51-4"></a>  <span class="dt">alpha =</span> <span class="dv">0</span>,               <span class="co"># ridge: alpha = 0</span></span>
<span><a rel="noopener" href="#cb51-5"></a>  <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span><a rel="noopener" href="#cb51-6"></a>    <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)  </span>
<span><a rel="noopener" href="#cb51-7"></a></span>
<span><a rel="noopener" href="#cb51-8"></a><span class="kw">plot</span>(cv_ridge)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="section level3">
<h3>Model Evaluation</h3>
<p>We now look at the ROC curve for a model using the optimal penalty parameter <span class="math inline">\(\lambda\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb52-1"></a>dfRidgeOpt &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span><a rel="noopener" href="#cb52-2"></a>  <span class="dt">pi =</span> <span class="kw">predict</span>(cv_ridge,</span>
<span><a rel="noopener" href="#cb52-3"></a>    <span class="dt">newx =</span> XTest,</span>
<span><a rel="noopener" href="#cb52-4"></a>    <span class="dt">s =</span> cv_ridge<span class="op">$</span>lambda.min,</span>
<span><a rel="noopener" href="#cb52-5"></a>    <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">c</span>(.),</span>
<span><a rel="noopener" href="#cb52-6"></a>  <span class="dt">known.truth =</span> YTest)</span>
<span><a rel="noopener" href="#cb52-7"></a></span>
<span><a rel="noopener" href="#cb52-8"></a></span>
<span><a rel="noopener" href="#cb52-9"></a>rocridge &lt;-</span>
<span><a rel="noopener" href="#cb52-10"></a><span class="st">  </span>dfRidgeOpt  <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb52-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">d =</span> known.truth, <span class="dt">m =</span> pi)) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb52-12"></a><span class="st">  </span><span class="kw">geom_roc</span>(<span class="dt">n.cuts =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb52-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb52-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span><a rel="noopener" href="#cb52-15"></a></span>
<span><a rel="noopener" href="#cb52-16"></a>rocridge</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb53-1"></a><span class="kw">calc_auc</span>(rocridge)</span></code></pre></div>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.8311037</code></pre>
</div>
</div>
<div class="section level2">
<h2>Lasso</h2>
<p>The penalize Lasso regression, like the Ridge regression, uses a penalty term, but a different one:</p>
<p><span class="math inline">\(SSE_{pen}=||Y-X\beta||^2_2 + \lambda||\beta||_1\)</span></p>
<p>with <span class="math inline">\(||\beta||_1= \sum_{j=1}^p|\beta|\)</span></p>
<p>The difference with the Ridge method is that instead of shrinking the <span class="math inline">\(\beta\)</span>’s towards zero, they will be set to zero, resulting in less parameters in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb55-1"></a>lasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span><a rel="noopener" href="#cb55-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span><a rel="noopener" href="#cb55-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span><a rel="noopener" href="#cb55-4"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,         <span class="co"># lasso: alpha = 1</span></span>
<span><a rel="noopener" href="#cb55-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)  </span>
<span><a rel="noopener" href="#cb55-6"></a></span>
<span><a rel="noopener" href="#cb55-7"></a><span class="kw">plot</span>(lasso, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.5</span>))</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb56-1"></a>cv_lasso &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(</span>
<span><a rel="noopener" href="#cb56-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span><a rel="noopener" href="#cb56-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span><a rel="noopener" href="#cb56-4"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,               <span class="co"># lasso: alpha = 1</span></span>
<span><a rel="noopener" href="#cb56-5"></a>  <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span><a rel="noopener" href="#cb56-6"></a>    <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)  </span>
<span><a rel="noopener" href="#cb56-7"></a></span>
<span><a rel="noopener" href="#cb56-8"></a><span class="kw">plot</span>(cv_lasso)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="section level3">
<h3>Model evaluation</h3>
<p>We now look at the ROC curve for a model using the optimal number of parameters and optimal penalty parameter <span class="math inline">\(\lambda\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb57-1"></a>cv_lasso<span class="op">$</span>lambda.min</span></code></pre></div>
<pre><code>## [1] 0.09765481</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb59-1"></a>dfLassoOpt &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span><a rel="noopener" href="#cb59-2"></a>  <span class="dt">pi =</span> <span class="kw">predict</span>(cv_lasso,</span>
<span><a rel="noopener" href="#cb59-3"></a>    <span class="dt">newx =</span> XTest,</span>
<span><a rel="noopener" href="#cb59-4"></a>    <span class="dt">s =</span> cv_lasso<span class="op">$</span>lambda.min,</span>
<span><a rel="noopener" href="#cb59-5"></a>    <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">c</span>(.),</span>
<span><a rel="noopener" href="#cb59-6"></a>  <span class="dt">known.truth =</span> YTest)</span>
<span><a rel="noopener" href="#cb59-7"></a></span>
<span><a rel="noopener" href="#cb59-8"></a></span>
<span><a rel="noopener" href="#cb59-9"></a>roclasso &lt;-</span>
<span><a rel="noopener" href="#cb59-10"></a><span class="st">  </span>dfLassoOpt  <span class="op">%&gt;%</span></span>
<span><a rel="noopener" href="#cb59-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">d =</span> known.truth, <span class="dt">m =</span> pi)) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb59-12"></a><span class="st">  </span><span class="kw">geom_roc</span>(<span class="dt">n.cuts =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb59-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb59-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>) <span class="op">+</span></span>
<span><a rel="noopener" href="#cb59-15"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>))<span class="op">+</span></span>
<span><a rel="noopener" href="#cb59-16"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>))</span>
<span><a rel="noopener" href="#cb59-17"></a></span>
<span><a rel="noopener" href="#cb59-18"></a>roclasso</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb60-1"></a><span class="kw">calc_auc</span>(roclasso)</span></code></pre></div>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.8536789</code></pre>
</div>
</div>
<div class="section level2">
<h2>Optimal Model</h2>
<p>We choose to continue with the Lasso model, as this model gives us a AUC of 0.85 (with an optimal <span class="math inline">\(\lambda\)</span> of 0.098 and 21 parameters) compared to Ridge where the AUC is a little less 0.83 (despite using all 13669 parameters). For PCR we had a high AUC, but as we tried to evaluate the model using the test data, it gave very bad results. This was probably because instead of manually selecting our PC’s, we had taken the first PC’s, taking the number that gave us the optimal AUC, not taking into account that these first PC’s do not necessarily carry the most information.</p>
</div>
<div class="section level2">
<h2>Choosing optimal threshold c for Lasso model</h2>
<p>We find the optimal c at a sensitivity rate of 0.77 and a 1-specificity of 0.19. This means we have an 85% chance of correctly calling a true positive, a 19% chance of a false positive and, consequently a 81% chance of correctly calling a negative. As we prefer being sure of our discoveries, and not waste time on false positives, we would like this to be rather low. Another option would be sensitivity of 85%, but then we have 27% chance on false positives, which would be rather high.</p>
</div>
</div>
<div class="section level1">
<h1>Conclusion</h1>
<p>We can assume with some confidence that there is a relation between rejection status and the gene expression of some genes. This could already be visualized with the LDA technique, and was also confirmed with hypothesis testing. We were able to produce a list of significant genes, with an FDR controlled at 10%, and additionally filtered this list with the use of local fdr. This gave us 522 highly significant genes which could be looked at further with relation to rejection status of transplanted kidneys. Additionally, we were able to produce a pretty good prediction model from this data set using the Lasso method, with a sensitivity of 80% and a specificity of 96%.</p>
</div>
<div class="section level1">
<h1>Appendix</h1>
<div class="section level2">
<h2>List of 522 most significant genes</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb62-1"></a>most_sig_p</span></code></pre></div>
<pre><code>##            adj_p                          ID         lfdr
## 1   7.425386e-13                  X1405_i_at 3.620658e-04
## 2   1.149318e-11               X1552701_a_at 1.165033e-03
## 3   4.415244e-12               X1552703_s_at 8.936532e-04
## 4   2.077969e-12               X1553906_s_at 5.719656e-04
## 5   1.094182e-11               X1554240_a_at 7.464837e-04
## 6   1.195978e-11               X1554899_s_at 1.414088e-03
## 7   2.683185e-11               X1555349_a_at 1.912053e-03
## 8   1.634780e-08               X1555600_s_at 1.910415e-02
## 9   2.053437e-08               X1555613_a_at 1.940284e-02
## 10  2.977248e-07               X1555638_a_at 8.233065e-02
## 11  2.824786e-12               X1555691_a_at 4.902828e-04
## 12  7.276777e-08               X1555728_a_at 5.117521e-02
## 13  1.103731e-10               X1555756_a_at 2.412096e-03
## 14  1.811396e-09               X1555812_a_at 1.012178e-02
## 15  1.356124e-15                 X1555852_at 6.238934e-05
## 16  3.714992e-12                 X1557116_at 4.655428e-04
## 17  1.010336e-08                 X1557236_at 1.395059e-02
## 18  6.017943e-08                 X1557551_at 3.561938e-02
## 19  5.260119e-09                 X1557632_at 1.479477e-02
## 20  2.456296e-07               X1558517_s_at 7.637395e-02
## 21  4.151473e-07                 X1558971_at 7.910510e-02
## 22  8.545219e-08               X1558972_s_at 4.056058e-02
## 23  3.887239e-09                 X1559018_at 1.286836e-02
## 24  9.734152e-08               X1559263_s_at 4.646866e-02
## 25  2.330548e-11               X1559584_a_at 1.741217e-03
## 26  2.241288e-07                 X1560156_at 6.434462e-02
## 27  1.182492e-08                 X1563357_at 2.096062e-02
## 28  2.119722e-08                 X1563509_at 2.061662e-02
## 29  1.966719e-08               X1565754_x_at 1.977271e-02
## 30  2.304930e-13                 X1568592_at 2.046656e-04
## 31  1.729592e-09                 X1568943_at 7.923111e-03
## 32  2.530978e-11               X1569225_a_at 1.080865e-03
## 33  4.715155e-07                 X1569788_at 8.354704e-02
## 34  3.579358e-07                 X1570165_at 8.524423e-02
## 35  8.603612e-12                X200628_s_at 6.709839e-04
## 36  1.181572e-13                  X200629_at 1.256821e-04
## 37  1.230194e-13                X200887_s_at 1.881034e-04
## 38  2.786089e-17                  X200904_at 1.402394e-05
## 39  1.104867e-14                X200905_x_at 1.107310e-04
## 40  4.578867e-15                X201137_s_at 1.127558e-04
## 41  1.541350e-10                  X201288_at 3.469074e-03
## 42  2.604186e-07                  X201641_at 8.491211e-02
## 43  5.578022e-13                  X201649_at 2.978952e-04
## 44  2.334487e-11                X201720_s_at 2.029553e-03
## 45  7.407131e-13                X201721_s_at 5.607425e-04
## 46  8.698171e-09                  X201743_at 2.144690e-02
## 47  6.207975e-10                  X201761_at 7.035994e-03
## 48  4.107987e-07                X201853_s_at 9.399762e-02
## 49  8.097218e-13                X201858_s_at 5.609876e-04
## 50  1.387966e-14                  X201859_at 1.688535e-04
## 51  1.895801e-08                  X201954_at 3.331926e-02
## 52  7.649775e-12                X202156_s_at 6.955145e-04
## 53  1.983399e-16                X202269_x_at 2.358677e-05
## 54  6.001809e-17                  X202270_at 1.846361e-05
## 55  2.986761e-16                X202307_s_at 2.329675e-05
## 56  2.910604e-07                X202510_s_at 9.375509e-02
## 57  1.624232e-13                  X202531_at 1.681143e-04
## 58  2.821336e-11                  X202625_at 1.654482e-03
## 59  2.493848e-11                X202626_s_at 1.350408e-03
## 60  2.959071e-08                X202638_s_at 3.132442e-02
## 61  8.975969e-12                X202643_s_at 9.644108e-04
## 62  5.306727e-14                X202644_s_at 1.677088e-04
## 63  2.176576e-13                  X202659_at 2.267655e-04
## 64  8.104611e-11                  X202663_at 2.121815e-03
## 65  4.932657e-17                  X202748_at 1.778493e-05
## 66  2.040380e-09                  X202794_at 1.180117e-02
## 67  3.567943e-07                  X202800_at 8.807433e-02
## 68  3.052273e-12                X202803_s_at 8.423128e-04
## 69  4.654386e-12                X202901_x_at 9.907669e-04
## 70  3.766101e-13                X202902_s_at 4.277927e-04
## 71  8.648123e-11                X202910_s_at 2.604452e-03
## 72  1.233872e-12                  X202953_at 5.260195e-04
## 73  3.522188e-15                  X202957_at 9.096600e-05
## 74  1.097388e-08                  X202963_at 1.735764e-02
## 75  1.225919e-09                  X203047_at 7.771101e-03
## 76  1.512702e-09                  X203052_at 7.408192e-03
## 77  5.790038e-10                  X203104_at 6.327364e-03
## 78  2.284907e-11                  X203185_at 1.734794e-03
## 79  3.822554e-11                X203217_s_at 1.437952e-03
## 80  5.329946e-08                X203299_s_at 4.194320e-02
## 81  6.561636e-08                X203300_x_at 4.912491e-02
## 82  1.479418e-09                  X203320_at 8.852269e-03
## 83  1.081217e-10                X203332_s_at 2.879021e-03
## 84  3.417715e-08                X203358_s_at 2.705177e-02
## 85  2.017556e-14                  X203416_at 1.628818e-04
## 86  8.317687e-13                X203471_s_at 4.269345e-04
## 87  7.018357e-12                  X203485_at 9.378472e-04
## 88  1.127257e-09                  X203561_at 9.211695e-03
## 89  1.971575e-09                X203645_s_at 1.076207e-02
## 90  3.417715e-08                  X203708_at 3.993061e-02
## 91  1.724532e-13                X203741_s_at 2.871221e-04
## 92  1.461838e-10                X203760_s_at 2.480856e-03
## 93  7.305201e-14                  X203761_at 1.355016e-04
## 94  8.514508e-17                  X203915_at 3.700306e-05
## 95  5.447549e-10                X203923_s_at 5.110540e-03
## 96  1.746824e-13                  X203932_at 2.324961e-04
## 97  4.959523e-13                X204006_s_at 4.411846e-04
## 98  8.419582e-12                  X204057_at 1.098890e-03
## 99  6.485410e-14                  X204070_at 1.346072e-04
## 100 3.123357e-15                  X204103_at 6.206301e-05
## 101 3.000464e-09                  X204116_at 1.170478e-02
## 102 1.669715e-12                  X204118_at 7.198605e-04
## 103 9.948463e-13                  X204122_at 5.965401e-04
## 104 1.661433e-07                  X204150_at 7.737368e-02
## 105 3.319462e-10                X204153_s_at 4.468845e-03
## 106 1.416223e-07                  X204162_at 5.460536e-02
## 107 3.128867e-08                  X204192_at 3.478678e-02
## 108 1.435720e-11                X204198_s_at 9.358360e-04
## 109 4.307637e-17                  X204205_at 1.656578e-05
## 110 7.457221e-14                  X204220_at 1.962447e-04
## 111 3.081106e-09                X204222_s_at 1.689498e-02
## 112 6.951201e-11                  X204232_at 2.580511e-03
## 113 4.981307e-12                  X204236_at 1.053088e-03
## 114 4.932657e-17                  X204279_at 1.745959e-05
## 115 8.623533e-10                X204285_s_at 5.395458e-03
## 116 3.772141e-13                X204319_s_at 3.755542e-04
## 117 3.533139e-07                X204416_x_at 9.718485e-02
## 118 6.400123e-10                  X204438_at 7.357422e-03
## 119 4.222375e-13                X204446_s_at 5.618933e-04
## 120 4.644095e-10                  X204502_at 4.846680e-03
## 121 1.587468e-10                X204529_s_at 2.981791e-03
## 122 5.047441e-15                  X204533_at 8.795770e-05
## 123 3.446811e-08                  X204563_at 3.212630e-02
## 124 1.669605e-14                  X204655_at 1.078309e-04
## 125 7.089078e-12                  X204661_at 1.359606e-03
## 126 1.846636e-16                X204670_x_at 4.672334e-05
## 127 1.667031e-14                  X204698_at 1.001435e-04
## 128 7.778373e-08                  X204747_at 4.796669e-02
## 129 6.914344e-10                  X204770_at 4.902441e-03
## 130 5.052163e-12                  X204774_at 1.078825e-03
## 131 6.546355e-08                  X204787_at 4.806883e-02
## 132 8.931841e-13                X204806_x_at 4.545863e-04
## 133 1.653294e-15                X204820_s_at 6.187259e-05
## 134 5.895852e-16                  X204821_at 3.501145e-05
## 135 1.481650e-12                  X204882_at 5.206417e-04
## 136 3.789796e-09                X204890_s_at 9.151458e-03
## 137 5.120117e-11                X204891_s_at 1.804281e-03
## 138 2.669784e-13                  X204912_at 2.948779e-04
## 139 1.831579e-09                  X204923_at 8.439576e-03
## 140 2.430960e-10                  X204924_at 3.621160e-03
## 141 2.433975e-09                  X204959_at 1.239233e-02
## 142 8.483898e-12                  X204971_at 1.253284e-03
## 143 3.237946e-07                  X204972_at 8.630459e-02
## 144 9.176924e-12                X205114_s_at 9.226004e-04
## 145 2.110090e-11                  X205159_at 1.711534e-03
## 146 3.406965e-09                  X205237_at 1.099518e-02
## 147 3.574365e-08                  X205242_at 2.735065e-02
## 148 4.095557e-16                  X205269_at 3.943378e-05
## 149 1.686451e-13                X205270_s_at 1.829686e-04
## 150 1.377410e-09                X205299_s_at 6.487657e-03
## 151 1.589198e-08                  X205419_at 2.670405e-02
## 152 1.085612e-15                  X205488_at 5.830991e-05
## 153 2.032449e-07                  X205569_at 6.119106e-02
## 154 5.261539e-07                  X205599_at 8.582737e-02
## 155 4.565742e-08                  X205668_at 4.615658e-02
## 156 2.314274e-09                X205671_s_at 9.241812e-03
## 157 2.370494e-11                  X205681_at 1.335447e-03
## 158 1.798157e-08                X205692_s_at 2.232959e-02
## 159 1.624232e-13                  X205758_at 1.405774e-04
## 160 1.283011e-07                X205786_s_at 7.096638e-02
## 161 4.703657e-09                X205804_s_at 1.340647e-02
## 162 5.010067e-15                  X205821_at 7.042824e-05
## 163 1.054450e-12                  X205831_at 4.270765e-04
## 164 4.439523e-07                  X205884_at 8.709068e-02
## 165 1.615840e-16                X205890_s_at 2.596978e-05
## 166 2.077969e-12                  X206011_at 7.011873e-04
## 167 6.480940e-07                X206060_s_at 9.722964e-02
## 168 2.328376e-15                  X206082_at 5.381255e-05
## 169 5.084150e-10                  X206118_at 4.739357e-03
## 170 6.399678e-12                  X206134_at 6.267692e-04
## 171 6.553667e-09                  X206150_at 1.333787e-02
## 172 2.585070e-13                X206366_x_at 2.161970e-04
## 173 8.317687e-13                  X206513_at 2.673239e-04
## 174 1.356319e-13                  X206584_at 3.242079e-04
## 175 8.437636e-08                  X206637_at 4.522399e-02
## 176 2.716318e-13                  X206666_at 2.707458e-04
## 177 2.186359e-07                  X206682_at 7.280125e-02
## 178 1.678252e-07                X206785_s_at 6.201167e-02
## 179 2.787903e-14                  X206914_at 7.412331e-05
## 180 1.050416e-07                  X206978_at 6.160008e-02
## 181 4.125618e-12                X207238_s_at 8.999483e-04
## 182 1.319362e-08                X207419_s_at 2.019044e-02
## 183 1.527256e-10                X207571_x_at 3.345542e-03
## 184 7.611830e-12                  X207651_at 7.515413e-04
## 185 3.534505e-08                X207691_x_at 3.413433e-02
## 186 5.389500e-11                  X207735_at 2.229923e-03
## 187 4.046305e-08                X207777_s_at 2.723526e-02
## 188 4.317801e-13                X207795_s_at 3.253197e-04
## 189 4.807518e-10                X207957_s_at 4.548133e-03
## 190 4.355221e-12                X208018_s_at 7.585386e-04
## 191 1.855774e-15                X208306_x_at 8.334484e-05
## 192 4.191974e-08                X208392_x_at 3.465291e-02
## 193 1.748109e-08                X208438_s_at 2.821431e-02
## 194 8.936184e-11                X208729_x_at 3.198541e-03
## 195 4.092067e-08                X208747_s_at 4.757231e-02
## 196 2.970280e-09                X208812_x_at 1.402361e-02
## 197 5.701295e-08                  X208829_at 3.652340e-02
## 198 5.893974e-11                  X208885_at 2.370789e-03
## 199 1.905855e-15                  X208894_at 9.256492e-05
## 200 2.668397e-15                X209040_s_at 7.405465e-05
## 201 1.109676e-11                  X209083_at 1.111284e-03
## 202 2.013791e-11                X209140_x_at 1.886099e-03
## 203 1.279802e-16                X209312_x_at 3.892429e-05
## 204 8.357093e-10                  X209473_at 7.087049e-03
## 205 4.267718e-09                X209474_s_at 1.298121e-02
## 206 5.798913e-11                X209546_s_at 1.901300e-03
## 207 1.459692e-12                  X209606_at 5.369917e-04
## 208 2.603487e-07                  X209619_at 8.791914e-02
## 209 4.316387e-11                  X209670_at 1.562258e-03
## 210 6.067047e-10                  X209683_at 5.029078e-03
## 211 8.628391e-11                X209685_s_at 2.325285e-03
## 212 9.695711e-12                  X209723_at 1.378994e-03
## 213 1.363314e-13                  X209732_at 2.614909e-04
## 214 5.747145e-11                  X209760_at 1.406086e-03
## 215 2.676039e-14                  X209770_at 1.108353e-04
## 216 1.231386e-12                  X209795_at 4.136619e-04
## 217 3.908980e-09                X209813_x_at 1.190329e-02
## 218 1.255825e-12                X209823_x_at 4.838219e-04
## 219 5.754985e-09                  X209829_at 1.507172e-02
## 220 1.566574e-11                X209846_s_at 1.325576e-03
## 221 1.435720e-11                  X209879_at 1.227768e-03
## 222 1.492936e-11                  X209906_at 1.403840e-03
## 223 4.887083e-11                  X209949_at 2.152153e-03
## 224 4.317801e-13                X209969_s_at 2.452813e-04
## 225 1.054450e-12                X209970_x_at 5.411788e-04
## 226 1.230194e-13                  X210029_at 1.539658e-04
## 227 3.185782e-12                  X210031_at 5.121560e-04
## 228 1.624232e-13                  X210163_at 1.872233e-04
## 229 5.802389e-09                  X210176_at 1.620130e-02
## 230 1.320180e-07                X210222_s_at 5.689750e-02
## 231 8.529108e-11                  X210279_at 1.957731e-03
## 232 3.458360e-10                X210514_x_at 5.211095e-03
## 233 2.595732e-14                X210606_x_at 1.283921e-04
## 234 4.435435e-10                X210644_s_at 4.888210e-03
## 235 9.437642e-10                X210754_s_at 6.556648e-03
## 236 1.076320e-09                X210785_s_at 7.479256e-03
## 237 2.047449e-13                X210895_s_at 2.843850e-04
## 238 2.493848e-11                X210915_x_at 1.510229e-03
## 239 7.096495e-11                X210972_x_at 1.952098e-03
## 240 1.653294e-15                X210982_s_at 8.655467e-05
## 241 2.208043e-14                X211122_s_at 1.267405e-04
## 242 5.439410e-09                X211144_x_at 1.398591e-02
## 243 1.250900e-10                X211339_s_at 1.989919e-03
## 244 5.569685e-12                X211366_x_at 9.514719e-04
## 245 1.367048e-12                X211367_s_at 6.039789e-04
## 246 1.025960e-12                X211368_s_at 5.948477e-04
## 247 4.807518e-10                X211528_x_at 6.094019e-03
## 248 2.087433e-10                X211529_x_at 4.345763e-03
## 249 3.857115e-09                X211530_x_at 1.290544e-02
## 250 2.439423e-08                X211654_x_at 3.322525e-02
## 251 9.150410e-12                X211656_x_at 1.052244e-03
## 252 8.975969e-12                X211742_s_at 1.429877e-03
## 253 2.652028e-07                X211762_s_at 8.936933e-02
## 254 4.642311e-08                X211795_s_at 3.355937e-02
## 255 1.550708e-11                X211796_s_at 1.307650e-03
## 256 1.082412e-10                X211799_x_at 2.893524e-03
## 257 2.402586e-07                X211822_s_at 7.744272e-02
## 258 4.572738e-09                X211824_x_at 1.215590e-02
## 259 1.011137e-09                X211911_x_at 8.305950e-03
## 260 2.827256e-12                  X211990_at 1.039627e-03
## 261 1.687625e-15                X211991_s_at 7.567568e-05
## 262 5.259616e-10                  X212288_at 4.915690e-03
## 263 1.684048e-07                X212486_s_at 7.838220e-02
## 264 1.469118e-10                X212587_s_at 3.324036e-03
## 265 1.140849e-13                  X212588_at 2.850332e-04
## 266 2.232458e-12                  X212613_at 4.718255e-04
## 267 1.255825e-12                X212671_s_at 6.105488e-04
## 268 4.267718e-09                X212796_s_at 1.408081e-02
## 269 8.419582e-12                  X212873_at 9.137613e-04
## 270 1.102844e-11                X212998_x_at 1.476317e-03
## 271 1.916147e-12                  X213160_at 5.382035e-04
## 272 7.344377e-11                X213193_x_at 2.162127e-03
## 273 3.477653e-07                  X213261_at 8.715232e-02
## 274 4.587482e-13                X213293_s_at 4.240159e-04
## 275 5.447549e-10                  X213326_at 6.151628e-03
## 276 5.146963e-14                  X213415_at 1.597528e-04
## 277 1.198088e-09                  X213416_at 8.309677e-03
## 278 8.975969e-12                  X213537_at 1.041298e-03
## 279 1.316555e-12                  X213566_at 7.094586e-04
## 280 4.706224e-12                X213603_s_at 8.372426e-04
## 281 2.721280e-11                  X213733_at 1.578633e-03
## 282 1.858068e-12                X213888_s_at 5.446484e-04
## 283 5.131171e-09                X213932_x_at 1.772509e-02
## 284 2.848789e-11                X213975_s_at 2.714317e-03
## 285 5.844381e-11                X214181_x_at 2.050591e-03
## 286 3.500833e-07                  X214405_at 7.716547e-02
## 287 3.329306e-09                X214459_x_at 1.402766e-02
## 288 9.418440e-11                  X214467_at 2.627635e-03
## 289 3.119874e-09                  X214470_at 1.531867e-02
## 290 5.710834e-12                X214511_x_at 6.901566e-04
## 291 3.157686e-13                X214567_s_at 2.269517e-04
## 292 4.445894e-10                X214574_x_at 5.641378e-03
## 293 2.236526e-13                  X214617_at 2.019825e-04
## 294 4.143236e-08                  X214770_at 4.109654e-02
## 295 2.158645e-10                X215049_x_at 4.370354e-03
## 296 5.966990e-08                  X215147_at 3.181570e-02
## 297 6.440643e-16                X215193_x_at 6.395560e-05
## 298 2.929112e-09                X215313_x_at 1.432486e-02
## 299 1.424612e-08                X215493_x_at 2.506180e-02
## 300 2.154189e-08                  X215602_at 2.368611e-02
## 301 3.822554e-11                X215806_x_at 1.642688e-03
## 302 4.745719e-08                X215933_s_at 3.795747e-02
## 303 6.236686e-08                X216191_s_at 3.919396e-02
## 304 1.059384e-08                X216231_s_at 2.822548e-02
## 305 1.288175e-08                X216526_x_at 2.607898e-02
## 306 7.186554e-11                  X216834_at 2.651564e-03
## 307 4.721071e-12                X216920_s_at 8.110948e-04
## 308 2.780893e-11                X216950_s_at 1.208251e-03
## 309 3.280884e-09                  X217028_at 1.400057e-02
## 310 4.383334e-09                X217143_s_at 1.335653e-02
## 311 8.279660e-08                X217147_s_at 4.051660e-02
## 312 1.686451e-13                X217362_x_at 2.559587e-04
## 313 1.582241e-10                X217436_x_at 3.478400e-03
## 314 7.885851e-14                X217456_x_at 2.259280e-04
## 315 5.302119e-15                X217478_s_at 8.698804e-05
## 316 6.424815e-08                X217733_s_at 5.935858e-02
## 317 3.130279e-08                X217763_s_at 3.928497e-02
## 318 1.359540e-08                X217838_s_at 2.124637e-02
## 319 8.003032e-12                X217933_s_at 6.410025e-04
## 320 1.839165e-07                  X217984_at 7.177031e-02
## 321 4.442235e-11                X218223_s_at 1.728915e-03
## 322 3.400193e-12                  X218232_at 7.928438e-04
## 323 1.172466e-12                X218322_s_at 4.581991e-04
## 324 4.255860e-09                X218543_s_at 1.392416e-02
## 325 3.508950e-12                X218559_s_at 8.747410e-04
## 326 6.239748e-10                  X218611_at 7.218849e-03
## 327 3.737146e-07                X218747_s_at 9.496789e-02
## 328 3.993949e-12                  X218764_at 6.193992e-04
## 329 2.563044e-12                  X218805_at 4.744214e-04
## 330 1.516866e-08                  X218870_at 2.753290e-02
## 331 2.960523e-12                  X219014_at 7.985022e-04
## 332 3.018445e-12                  X219033_at 8.471609e-04
## 333 8.539644e-13                X219191_s_at 3.601617e-04
## 334 1.153896e-15                  X219243_at 4.547638e-05
## 335 5.837992e-08                  X219279_at 4.334651e-02
## 336 5.341423e-13                X219386_s_at 3.131897e-04
## 337 2.047873e-07                X219423_x_at 7.015542e-02
## 338 1.787712e-08                  X219471_at 2.608839e-02
## 339 1.028498e-09                X219528_s_at 6.181349e-03
## 340 1.396640e-09                  X219563_at 9.613935e-03
## 341 3.859814e-08                  X219574_at 3.579364e-02
## 342 7.560609e-10                  X219584_at 7.016462e-03
## 343 7.022390e-10                X219607_s_at 8.584704e-03
## 344 3.052273e-12                  X219666_at 8.206962e-04
## 345 2.868549e-08                  X219684_at 3.613482e-02
## 346 7.622696e-08                X219821_s_at 5.114181e-02
## 347 1.678252e-07                  X219892_at 7.589487e-02
## 348 3.339871e-09                X219938_s_at 9.093534e-03
## 349 1.527256e-10                  X220005_at 2.727752e-03
## 350 4.796143e-08                X220132_s_at 3.311337e-02
## 351 1.258076e-09                  X220146_at 7.341256e-03
## 352 1.160128e-09                X220330_s_at 9.512968e-03
## 353 2.694372e-09                  X220351_at 7.260923e-03
## 354 5.844709e-10                  X220704_at 4.228072e-03
## 355 4.736020e-08                X221601_s_at 3.544311e-02
## 356 2.800000e-12                X221653_x_at 4.385362e-04
## 357 9.464362e-13                X221698_s_at 4.590950e-04
## 358 5.120117e-11                  X221840_at 2.142811e-03
## 359 2.361495e-14                X221875_x_at 1.482041e-04
## 360 8.305069e-14                  X221978_at 1.709507e-04
## 361 4.174515e-07                  X222062_at 9.228630e-02
## 362 1.962803e-07                X222218_s_at 6.678356e-02
## 363 4.387748e-12                X222592_s_at 7.551620e-04
## 364 2.824786e-12                  X222838_at 6.769866e-04
## 365 3.412877e-07                X222859_s_at 7.663108e-02
## 366 7.033363e-10                X222895_s_at 4.487593e-03
## 367 1.129855e-09                X223220_s_at 8.224808e-03
## 368 1.211745e-11                X223280_x_at 1.343045e-03
## 369 3.805337e-12                  X223303_at 7.219853e-04
## 370 7.523803e-13                  X223322_at 3.784953e-04
## 371 1.220268e-08                X223344_s_at 2.515460e-02
## 372 1.195566e-11                  X223501_at 9.582840e-04
## 373 8.010296e-09                  X223533_at 1.737122e-02
## 374 4.440909e-07                  X223586_at 9.999133e-02
## 375 8.936184e-11                X223922_x_at 2.673392e-03
## 376 6.207975e-10                X223980_s_at 5.680075e-03
## 377 5.492605e-12                X224356_x_at 1.006016e-03
## 378 1.332427e-12                X224451_x_at 4.061661e-04
## 379 8.140815e-08                  X224701_at 4.674738e-02
## 380 1.320180e-07                  X224796_at 6.852428e-02
## 381 6.886780e-10                X225353_s_at 6.688911e-03
## 382 7.714022e-09                  X225618_at 1.838327e-02
## 383 2.654425e-10                  X225636_at 4.019676e-03
## 384 3.564038e-09                  X225701_at 1.091187e-02
## 385 8.097218e-13                  X225763_at 2.944312e-04
## 386 1.583501e-07                X225929_s_at 5.740868e-02
## 387 1.085612e-15                  X225973_at 4.704172e-05
## 388 6.155055e-09                  X226218_at 2.010913e-02
## 389 4.247358e-13                  X226219_at 3.030626e-04
## 390 8.437636e-08                  X226372_at 5.098036e-02
## 391 2.223055e-10                  X226459_at 3.700955e-03
## 392 2.097251e-17                  X226474_at 8.833752e-06
## 393 5.447549e-10                  X226525_at 6.164314e-03
## 394 1.471770e-08                  X226659_at 1.993422e-02
## 395 6.134428e-08                  X226725_at 4.323557e-02
## 396 4.642311e-08                  X226743_at 4.302174e-02
## 397 1.062207e-08                  X226771_at 2.585478e-02
## 398 2.058885e-08                  X226789_at 2.815950e-02
## 399 7.876642e-13                  X226818_at 5.759077e-04
## 400 4.574579e-12                  X226841_at 1.044329e-03
## 401 5.804978e-15                  X226878_at 6.611053e-05
## 402 4.374950e-14                  X226991_at 1.324640e-04
## 403 1.300091e-08                  X227125_at 2.439472e-02
## 404 6.678506e-11                  X227178_at 2.342502e-03
## 405 3.766101e-13                  X227344_at 3.060348e-04
## 406 3.407981e-13                  X227346_at 3.075847e-04
## 407 5.926668e-11                  X227353_at 1.951237e-03
## 408 1.255825e-12                  X227458_at 2.795692e-04
## 409 3.677731e-10                  X227609_at 4.081180e-03
## 410 8.419582e-12                  X227645_at 9.325269e-04
## 411 2.129163e-08                  X227677_at 2.402402e-02
## 412 5.760748e-10                  X227807_at 5.287283e-03
## 413 1.831579e-09                  X228094_at 8.706209e-03
## 414 1.515073e-07                  X228167_at 6.040138e-02
## 415 2.239283e-10                  X228273_at 3.266728e-03
## 416 8.097218e-13                  X228376_at 5.400908e-04
## 417 1.421590e-08                  X228410_at 2.395946e-02
## 418 3.089281e-12                  X228442_at 5.962820e-04
## 419 1.509039e-07                  X228471_at 6.007388e-02
## 420 1.022698e-12                  X228532_at 5.615032e-04
## 421 1.776049e-08                  X228826_at 1.782524e-02
## 422 3.349550e-09                  X228869_at 1.033462e-02
## 423 1.005245e-08                  X228964_at 2.150697e-02
## 424 8.213069e-11                X229041_s_at 2.180647e-03
## 425 7.349068e-08                X229120_s_at 5.984971e-02
## 426 1.846636e-16                X229367_s_at 2.985189e-05
## 427 3.315746e-16                  X229390_at 3.888147e-05
## 428 3.522188e-15                X229391_s_at 6.397387e-05
## 429 1.458552e-13                  X229437_at 1.274432e-04
## 430 3.289289e-08                  X229450_at 3.583515e-02
## 431 8.965464e-13                  X229560_at 3.739805e-04
## 432 3.543660e-15                  X229625_at 4.516297e-05
## 433 1.115748e-11                  X229629_at 8.088926e-04
## 434 1.211745e-11                  X229670_at 1.053617e-03
## 435 3.406965e-09                  X229686_at 1.089627e-02
## 436 6.270806e-14                  X229723_at 1.716042e-04
## 437 1.579102e-08                  X229934_at 2.148729e-02
## 438 1.455256e-11                X229937_x_at 1.048221e-03
## 439 8.835041e-08                  X229968_at 5.212837e-02
## 440 7.276777e-08                  X230110_at 4.277612e-02
## 441 3.508950e-12                  X230391_at 7.116446e-04
## 442 8.936184e-11                  X230405_at 2.450262e-03
## 443 1.264592e-13                  X230550_at 2.357941e-04
## 444 3.212024e-13                  X230741_at 2.714579e-04
## 445 5.182511e-08                  X230748_at 4.190396e-02
## 446 1.396640e-09                  X230753_at 7.311810e-03
## 447 1.491484e-09                  X230836_at 6.808945e-03
## 448 8.376351e-11                  X230925_at 3.141413e-03
## 449 4.611798e-10                  X231093_at 3.761336e-03
## 450 3.771148e-07                  X231165_at 9.181596e-02
## 451 4.248983e-11                  X231241_at 1.438823e-03
## 452 4.307637e-17                X231577_s_at 1.479782e-05
## 453 1.415354e-08                  X231578_at 1.350868e-02
## 454 4.808601e-09                  X231747_at 1.488216e-02
## 455 3.788873e-16                  X232024_at 4.728764e-05
## 456 7.619725e-12                  X232375_at 7.331966e-04
## 457 1.344821e-09                  X232504_at 5.618944e-03
## 458 2.844540e-07                  X232521_at 7.903299e-02
## 459 4.141944e-11                X232543_x_at 1.516650e-03
## 460 8.964382e-15                  X232617_at 1.262878e-04
## 461 1.648612e-09                  X233411_at 7.483416e-03
## 462 3.705979e-08                X233500_x_at 2.785495e-02
## 463 9.168422e-08                X233510_s_at 4.592545e-02
## 464 5.176053e-08                  X233690_at 4.009789e-02
## 465 2.889762e-07                  X234260_at 8.264139e-02
## 466 5.544378e-14                  X234987_at 2.236951e-04
## 467 4.703657e-09                  X235157_at 1.289831e-02
## 468 1.284485e-13                  X235175_at 1.398328e-04
## 469 8.132646e-11                  X235199_at 2.546212e-03
## 470 4.137211e-15                  X235229_at 4.947134e-05
## 471 8.289542e-08                  X235230_at 4.699998e-02
## 472 8.907455e-09                  X235276_at 1.850030e-02
## 473 2.669784e-13                X235529_x_at 3.157679e-04
## 474 3.420906e-08                  X235574_at 2.640304e-02
## 475 8.437636e-08                  X235735_at 3.811968e-02
## 476 3.152223e-14                X235964_x_at 1.714831e-04
## 477 1.615587e-07                  X236226_at 4.748774e-02
## 478 2.018954e-09                  X236280_at 7.480346e-03
## 479 5.332907e-09                  X236293_at 1.156294e-02
## 480 9.464362e-13                X236295_s_at 3.270151e-04
## 481 2.780893e-11                  X236401_at 1.455940e-03
## 482 3.870758e-07                  X236539_at 8.356385e-02
## 483 2.146578e-11                  X236782_at 1.362672e-03
## 484 1.261642e-08                  X236921_at 1.941951e-02
## 485 2.910553e-10                  X237104_at 3.223908e-03
## 486 3.406965e-09                  X237759_at 9.389482e-03
## 487 3.489266e-11                  X238025_at 1.599816e-03
## 488 6.292165e-08                  X238376_at 4.163151e-02
## 489 2.183002e-09                  X238439_at 6.819116e-03
## 490 8.936184e-11                X238531_x_at 1.222422e-03
## 491 1.667738e-12                  X238581_at 2.812780e-04
## 492 3.157646e-13                  X238668_at 3.136289e-04
## 493 9.902488e-15                  X238725_at 6.890438e-05
## 494 2.047873e-07                  X239108_at 6.779345e-02
## 495 3.339871e-09                  X239237_at 9.883859e-03
## 496 3.993949e-12                  X239294_at 7.628496e-04
## 497 1.724023e-07                  X239317_at 6.809640e-02
## 498 1.223285e-09                  X239946_at 6.815229e-03
## 499 5.646274e-10                  X239979_at 5.317870e-03
## 500 2.855177e-07                  X240154_at 8.489202e-02
## 501 6.890032e-08                  X240481_at 3.625960e-02
## 502 2.358277e-10                  X240665_at 3.071295e-03
## 503 1.336371e-12                  X241869_at 4.427988e-04
## 504 2.110090e-11                  X241891_at 1.236353e-03
## 505 1.302897e-10                  X242268_at 2.939413e-03
## 506 1.908649e-11                  X242521_at 1.451331e-03
## 507 5.447549e-10                  X242814_at 4.137450e-03
## 508 1.653294e-15                  X242907_at 4.188091e-05
## 509 8.169267e-08                  X242946_at 3.759926e-02
## 510 7.349068e-08                  X243271_at 4.385248e-02
## 511 9.416730e-09                X243366_s_at 1.935353e-02
## 512 2.146469e-09                  X243819_at 1.027203e-02
## 513 2.187554e-09                  X244352_at 8.971601e-03
## 514 7.735013e-09                  X244375_at 1.470252e-02
## 515 2.317243e-14                  X244598_at 8.377515e-05
## 516 4.645926e-13                   X34210_at 5.459158e-04
## 517 1.847455e-15                   X38241_at 5.797876e-05
## 518 2.380864e-08                 X44790_s_at 2.686205e-02
## 519 3.006905e-07                   X91703_at 8.426189e-02
## 520 2.669784e-13  AFFX.HUMISGF3A.M97935_3_at 2.350031e-04
## 521 5.275697e-10 AFFX.HUMISGF3A.M97935_MA_at 4.434232e-03
## 522 1.494010e-11 AFFX.HUMISGF3A.M97935_MB_at 1.054755e-03</code></pre>
</div>
<div class="section level2">
<h2>PCR</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb64-1"></a><span class="co">## Separate response and predictor variables</span></span>
<span><a rel="noopener" href="#cb64-2"></a>y &lt;-<span class="st"> </span><span class="kw">factor</span>(kidney_data<span class="op">$</span>Reject_Status)</span>
<span><a rel="noopener" href="#cb64-3"></a>X &lt;-<span class="st"> </span>X &lt;-<span class="st"> </span>kidney_data <span class="op">%&gt;%</span><span class="st"> </span></span>
<span><a rel="noopener" href="#cb64-4"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>Patient_ID, <span class="op">-</span>Reject_Status) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span><a rel="noopener" href="#cb64-5"></a><span class="st">  </span><span class="kw">as.matrix</span>()</span>
<span><a rel="noopener" href="#cb64-6"></a><span class="kw">dim</span>(X)</span></code></pre></div>
<pre><code>## [1]   250 13669</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb66-1"></a><span class="co">## Calculate PCA and extract scores</span></span>
<span><a rel="noopener" href="#cb66-2"></a>pca_X &lt;-<span class="st"> </span><span class="kw">prcomp</span>(X)</span>
<span><a rel="noopener" href="#cb66-3"></a>pca_var &lt;-<span class="st"> </span>pca_X<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></span>
<span><a rel="noopener" href="#cb66-4"></a>pca_var_per &lt;-<span class="st"> </span><span class="kw">round</span>(pca_var<span class="op">/</span><span class="kw">sum</span>(pca_var)<span class="op">*</span><span class="dv">100</span>, <span class="dv">1</span>)</span>
<span><a rel="noopener" href="#cb66-5"></a><span class="kw">barplot</span>(pca_var)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb67-1"></a>Z &lt;-<span class="st"> </span>pca_X<span class="op">$</span>x</span>
<span><a rel="noopener" href="#cb67-2"></a></span>
<span><a rel="noopener" href="#cb67-3"></a><span class="co">## Total number of available PCs</span></span>
<span><a rel="noopener" href="#cb67-4"></a>n_PC &lt;-<span class="st"> </span><span class="kw">ncol</span>(Z)</span>
<span><a rel="noopener" href="#cb67-5"></a></span>
<span><a rel="noopener" href="#cb67-6"></a><span class="co">## cv.glm() requires the response and predictors in one data.frame, so we need</span></span>
<span><a rel="noopener" href="#cb67-7"></a><span class="co">## to combine them back together</span></span>
<span><a rel="noopener" href="#cb67-8"></a>fit_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(y, Z)</span>
<span><a rel="noopener" href="#cb67-9"></a><span class="co">#head(fit_data)</span></span>
<span><a rel="noopener" href="#cb67-10"></a></span>
<span><a rel="noopener" href="#cb67-11"></a><span class="co">## Example of PC Log. Reg. with all PCs</span></span>
<span><a rel="noopener" href="#cb67-12"></a>full_model &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> fit_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb69-1"></a><span class="co"># summary(full_model)</span></span>
<span><a rel="noopener" href="#cb69-2"></a></span>
<span><a rel="noopener" href="#cb69-3"></a><span class="co">## 4-fold Cross-validation on this one particular model, using AUC and K = 4</span></span>
<span><a rel="noopener" href="#cb69-4"></a>full_model_cv &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(</span>
<span><a rel="noopener" href="#cb69-5"></a>  <span class="dt">data =</span> fit_data,  <span class="dt">glmfit =</span> full_model,</span>
<span><a rel="noopener" href="#cb69-6"></a>  <span class="dt">cost =</span> pROC<span class="op">::</span>auc, <span class="dt">K =</span> <span class="dv">4</span>  <span class="co"># note: specify the auc function (from pROC) without`()`!</span></span>
<span><a rel="noopener" href="#cb69-7"></a>)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb89-1"></a><span class="co">## We&#39;ll just use the raw one here</span></span>
<span><a rel="noopener" href="#cb89-2"></a>full_model_cv<span class="op">$</span>delta[<span class="dv">1</span>] <span class="co"># This is the AUC for this particular model estimated by AUC</span></span></code></pre></div>
<pre><code>## [1] 0.5333843</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb91-1"></a><span class="co">## wrap this code in a for-loop and repeat for each number of PCs</span></span>
<span><a rel="noopener" href="#cb91-2"></a>cv_auc &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, n_PC)</span>
<span><a rel="noopener" href="#cb91-3"></a><span class="kw">set.seed</span>(<span class="dv">12</span>) <span class="co"># seed for reproducibility</span></span>
<span><a rel="noopener" href="#cb91-4"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n_PC)) {</span>
<span><a rel="noopener" href="#cb91-5"></a>  <span class="co">## Prepare fit_data; subset number of PCs to i</span></span>
<span><a rel="noopener" href="#cb91-6"></a>  fit_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(y, Z[, <span class="dv">1</span><span class="op">:</span>i, <span class="dt">drop =</span> <span class="ot">FALSE</span>])  <span class="co"># use drop = FALSE to avoid problems when subsetting single column</span></span>
<span><a rel="noopener" href="#cb91-7"></a>  pcr_mod &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(</span>
<span><a rel="noopener" href="#cb91-8"></a>    <span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> fit_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span><a rel="noopener" href="#cb91-9"></a>  )</span>
<span><a rel="noopener" href="#cb91-10"></a>  </span>
<span><a rel="noopener" href="#cb91-11"></a> <span class="co">## Do 4-fold CV while suppressing Warnings and Messages </span></span>
<span><a rel="noopener" href="#cb91-12"></a>  cv &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(</span>
<span><a rel="noopener" href="#cb91-13"></a>    <span class="kw">suppressMessages</span>(</span>
<span><a rel="noopener" href="#cb91-14"></a>      <span class="kw">cv.glm</span>(fit_data, pcr_mod, <span class="dt">cost =</span> pROC<span class="op">::</span>auc, <span class="dt">K =</span> <span class="dv">4</span>)</span>
<span><a rel="noopener" href="#cb91-15"></a>    )</span>
<span><a rel="noopener" href="#cb91-16"></a>  )</span>
<span><a rel="noopener" href="#cb91-17"></a>  cv_auc[i] &lt;-<span class="st"> </span>cv<span class="op">$</span>delta[<span class="dv">1</span>]</span>
<span><a rel="noopener" href="#cb91-18"></a>}</span>
<span><a rel="noopener" href="#cb91-19"></a><span class="kw">names</span>(cv_auc) &lt;-<span class="st"> </span><span class="kw">seq_along</span>(cv_auc)</span>
<span><a rel="noopener" href="#cb91-20"></a><span class="co">#cv_auc</span></span>
<span><a rel="noopener" href="#cb91-21"></a><span class="co">## Finding the optimal nr. of PCs corresponds to finding the max. AUC</span></span>
<span><a rel="noopener" href="#cb91-22"></a>optim_nPC &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which.max</span>(cv_auc))</span>
<span><a rel="noopener" href="#cb91-23"></a>optim_nPC</span></code></pre></div>
<pre><code>## [1] &quot;13&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb93-1"></a><span class="kw">plot</span>(<span class="kw">names</span>(cv_auc), cv_auc, <span class="dt">xlab =</span> <span class="st">&quot;n PCs&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;AUC&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span><a rel="noopener" href="#cb93-2"></a><span class="kw">abline</span>(<span class="dt">v =</span> optim_nPC, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="section level3">
<h3>PCR prediction using Test data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb94-1"></a><span class="co">## prediction using Test data</span></span>
<span><a rel="noopener" href="#cb94-2"></a>pca_X &lt;-<span class="st"> </span><span class="kw">prcomp</span>(XTrain)</span>
<span><a rel="noopener" href="#cb94-3"></a><span class="co"># YTrain &lt;- as.factor(YTrain)</span></span>
<span><a rel="noopener" href="#cb94-4"></a><span class="co"># dim(Xtrain)</span></span>
<span><a rel="noopener" href="#cb94-5"></a>Z &lt;-<span class="st"> </span>pca_X<span class="op">$</span>x</span>
<span><a rel="noopener" href="#cb94-6"></a><span class="kw">dim</span>(Z)</span></code></pre></div>
<pre><code>## [1] 175 175</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb96-1"></a>pca_var &lt;-<span class="st"> </span>pca_X<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></span>
<span><a rel="noopener" href="#cb96-2"></a>pca_var_per &lt;-<span class="st"> </span><span class="kw">round</span>(pca_var<span class="op">/</span><span class="kw">sum</span>(pca_var)<span class="op">*</span><span class="dv">100</span>, <span class="dv">1</span>)</span>
<span><a rel="noopener" href="#cb96-3"></a><span class="kw">barplot</span>(pca_var)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb97-1"></a>opt_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(YTrain, Z[, <span class="dv">1</span><span class="op">:</span><span class="dv">13</span>])</span>
<span><a rel="noopener" href="#cb97-2"></a><span class="kw">head</span>(opt_data)</span></code></pre></div>
<pre><code>##           YTrain        PC1        PC2        PC3        PC4       PC5
## GSM534071      0 -60.292733  -4.556089   3.371708 -10.040185  7.630482
## GSM534109      1  74.356529 -10.279510  25.149007  -2.046462 14.189032
## GSM534117      0 -21.906103  16.704598 -16.938853 -10.004579 26.137068
## GSM534131      0  -6.608238  25.589412 -27.259152 -27.185315 23.709823
## GSM534182      1  29.749560 -49.655187 -39.576541   2.619380 -3.513022
## GSM534078      0 -31.480857 -42.825193  29.557156 -28.912738 35.277438
##                 PC6        PC7        PC8        PC9       PC10       PC11
## GSM534071  -1.94807   5.733308 -10.024387   5.361360  -3.795702  -1.486253
## GSM534109 -18.45390  15.352199 -31.022865 -14.304288   6.541908   2.841920
## GSM534117  22.87182   4.008890   3.466418   2.646779  35.090187   1.347568
## GSM534131  10.01935   4.115296  -3.215226  -1.867358  27.829806 -13.374546
## GSM534182  22.55835 -15.386190 -20.279292 -12.623975  -3.716419 -15.585971
## GSM534078 -16.11911  10.226272 -11.036991  -3.681144 -11.753402  21.595757
##                  PC12       PC13
## GSM534071   4.6602424  -1.916568
## GSM534109  -4.6349427   2.008916
## GSM534117 -13.1095755   4.655711
## GSM534131   1.8861148  -4.280373
## GSM534182   9.0369863 -20.219189
## GSM534078  -0.1575127 -11.606708</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb99-1"></a><span class="kw">table</span>(YTrain)</span></code></pre></div>
<pre><code>## YTrain
##   0   1 
## 122  53</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb101-1"></a><span class="kw">summary</span>(YTrain)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.3029  1.0000  1.0000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb103-1"></a>opt_model &lt;-<span class="st"> </span><span class="kw">glm</span>(YTrain <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> opt_data, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span><a rel="noopener" href="#cb103-2"></a><span class="kw">summary</span>(opt_model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = YTrain ~ ., family = &quot;binomial&quot;, data = opt_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2018  -0.6305  -0.3450   0.4490   2.2339  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.251801   0.234618  -5.335 9.53e-08 ***
## PC1          0.008228   0.005361   1.535 0.124786    
## PC2         -0.032518   0.007562  -4.300 1.71e-05 ***
## PC3         -0.029760   0.009481  -3.139 0.001696 ** 
## PC4          0.034602   0.010387   3.331 0.000864 ***
## PC5          0.013780   0.010791   1.277 0.201629    
## PC6          0.017617   0.011514   1.530 0.125996    
## PC7         -0.016490   0.011741  -1.404 0.160184    
## PC8         -0.016582   0.012676  -1.308 0.190832    
## PC9         -0.054119   0.016367  -3.307 0.000944 ***
## PC10        -0.014821   0.013334  -1.112 0.266336    
## PC11        -0.031991   0.015170  -2.109 0.034965 *  
## PC12         0.013823   0.015637   0.884 0.376697    
## PC13         0.005134   0.015646   0.328 0.742828    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 214.64  on 174  degrees of freedom
## Residual deviance: 142.35  on 161  degrees of freedom
## AIC: 170.35
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb105-1"></a><span class="co">#The estimators of coefficients that have been obtained (βZ), as stated in the introduction can be multiplied by matrix V to obtain βX.</span></span>
<span><a rel="noopener" href="#cb105-2"></a>beta.Z &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(opt_model<span class="op">$</span>coefficients[<span class="dv">2</span><span class="op">:</span><span class="dv">14</span>])</span>
<span><a rel="noopener" href="#cb105-3"></a>V &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(pca_X<span class="op">$</span>rotation[,<span class="dv">2</span><span class="op">:</span><span class="dv">14</span>])</span>
<span><a rel="noopener" href="#cb105-4"></a></span>
<span><a rel="noopener" href="#cb105-5"></a><span class="co"># In order to compare the prediction, I am predicting the values based on the βX coefficient estimates calculated before, according to this equation</span></span>
<span><a rel="noopener" href="#cb105-6"></a><span class="co"># βX = V X βZ</span></span>
<span><a rel="noopener" href="#cb105-7"></a>beta.X &lt;-<span class="st"> </span>V <span class="op">%*%</span><span class="st"> </span>beta.Z</span>
<span><a rel="noopener" href="#cb105-8"></a><span class="co">#head(beta.X)</span></span>
<span><a rel="noopener" href="#cb105-9"></a>pred.test &lt;-<span class="st"> </span>XTest</span>
<span><a rel="noopener" href="#cb105-10"></a><span class="co">#str(pred.test)</span></span>
<span><a rel="noopener" href="#cb105-11"></a><span class="co">#str(beta.X)</span></span>
<span><a rel="noopener" href="#cb105-12"></a>y.pred.test2 &lt;-<span class="st"> </span>pred.test <span class="op">%*%</span><span class="st"> </span>beta.X</span>
<span><a rel="noopener" href="#cb105-13"></a></span>
<span><a rel="noopener" href="#cb105-14"></a><span class="kw">plot</span>(y.pred.test2)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb106-1"></a>logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(logit){</span>
<span><a rel="noopener" href="#cb106-2"></a>  odds &lt;-<span class="st"> </span><span class="kw">exp</span>(logit)</span>
<span><a rel="noopener" href="#cb106-3"></a>  prob &lt;-<span class="st"> </span>odds <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>odds)</span>
<span><a rel="noopener" href="#cb106-4"></a>  <span class="kw">return</span>(prob)</span>
<span><a rel="noopener" href="#cb106-5"></a>}</span>
<span><a rel="noopener" href="#cb106-6"></a></span>
<span><a rel="noopener" href="#cb106-7"></a>prob &lt;-<span class="st"> </span><span class="kw">logit2prob</span>(y.pred.test2)</span>
<span><a rel="noopener" href="#cb106-8"></a>XTest<span class="op">$</span>predict &lt;-<span class="st"> </span><span class="kw">ifelse</span>(prob <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="st">&quot;1&quot;</span>, <span class="st">&quot;0&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in XTest$predict &lt;- ifelse(prob &gt; 0.5, &quot;1&quot;, &quot;0&quot;): Coercing LHS to a list</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb108-1"></a><span class="kw">table</span>(YTest)</span></code></pre></div>
<pre><code>## YTest
##  0  1 
## 52 23</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb110-1"></a>confusion_mat &lt;-<span class="st"> </span><span class="kw">table</span>(YTest, XTest<span class="op">$</span>predict)</span>
<span><a rel="noopener" href="#cb110-2"></a><span class="kw">rownames</span>(confusion_mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;obs.0&quot;</span>, <span class="st">&quot;obs.1&quot;</span>)</span>
<span><a rel="noopener" href="#cb110-3"></a><span class="kw">colnames</span>(confusion_mat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;pred.0&quot;</span>, <span class="st">&quot;pred.1&quot;</span>)</span>
<span><a rel="noopener" href="#cb110-4"></a>confusion_mat</span></code></pre></div>
<pre><code>##        
## YTest   pred.0 pred.1
##   obs.0     26     26
##   obs.1     12     11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb112-1"></a><span class="kw">dim</span>(<span class="kw">data.frame</span>(prob))</span></code></pre></div>
<pre><code>## [1] 75  1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb114-1"></a>pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob, YTest)</span>
<span><a rel="noopener" href="#cb114-2"></a></span>
<span><a rel="noopener" href="#cb114-3"></a>perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)</span>
<span><a rel="noopener" href="#cb114-4"></a><span class="kw">plot</span>(perf)</span></code></pre></div>
<p><img src="javascript://" width="672"/></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span><a rel="noopener" href="#cb115-1"></a>auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span><a rel="noopener" href="#cb115-2"></a>auc &lt;-<span class="st"> </span>auc<span class="op">@</span>y.values[[<span class="dv">1</span>]]</span>
<span><a rel="noopener" href="#cb115-3"></a><span class="kw">paste</span>(<span class="st">&quot;AUC:&quot;</span>, auc)</span></code></pre></div>
<pre><code>## [1] &quot;AUC: 0.48494983277592&quot;</code></pre>
</div>
</div>
</div>

<div>---
title: "Project Analysis of High Dimensional Data"
author: "Bhanu Durganath Angam, Birgit Deboutte, Narendra Kamineni"
date: "`r Sys.Date()`"
output:
  html_document:
      code_download: true
      theme: flatly
      toc: true
      toc_float: true
      highlight: tango
    
---

```{r,echo=FALSE}
suppressPackageStartupMessages({
  library(readr)
  library(dplyr)
  library(ggplot2)
  library(PMA)
  library(MASS)
  library(gridExtra)
  library(sparseLDA)
  library(locfdr)
  library(glmnet)
  library(plotROC)
  library(boot)
  library(pROC)
  library(ROCR)
})
```

# Summary

In order to find a possible relation between the rejection status of transplanted kidneys and gene expression we looked at a data set of 13699 genes for 250 patients. On visualizing the data we can see that there is on average a difference in gene expression between genes from a patient with rejected kidneys compared to no rejection. In order to explore this further, t-tests were performed for all 13699 genes to see if they were differentially expressed. Running so many tests simultaneously will cause a multiple testing problem, and will return many false positives. We set the false discovery rate at 10%, meaning that we accepted on average a 10% chance of the tests coming up with a false positive. After correcting for the multiple testing problem, we found 3106 genes to show differential expression. We filtered these results additionally using a local false discovery rate in order to find the most promising genes. We then had 522 genes with a very low probability of being false positives. Next, we tried to find a prediction model that could predict the rejection status from gene expression. The original data set was subset in a training set and a test set. We tried out three different prediction model building methods, using the training data set, obtained the optimal model for each of these methods and evaluated these models with the test data set in terms of sensitivity and specificity. The Lasso model gave the best results, giving us a prediction model with a sensitivity of 0.77 and a specificity of 0.81. 


# Data set

The purpose of this project is to find a possible relation between the rejection status of transplanted kidneys and gene expression. We will work with a subset of 13669 genes for 25O patients, 174 of these patients did not reject their transplanted kidneys, 76 did reject them. The 13669 genes are the 25% most variable genes of a complete set of 54675 genes.

```{r download-data, include=FALSE}
# Create raw-data/ folder if it does not exist (you can change this to whatever path you want)
out_path <- "raw-data"
if (!(dir.exists(out_path))) dir.create(out_path)

# Download data (only if doesn't exist locally)
# Delete existing file with `unlink(file.path(out_path, fname))`
# to force re-downloading
fname <- "GSE21374-kidney-data.csv.gz"
if (!file.exists(file.path(out_path, fname))) {
  data_url <-
    "https://github.com/statOmics/HDA2020/raw/data/GSE21374-kidney-data.csv.gz"
  download.file(data_url, destfile = file.path(out_path, fname))
}
```

```{r load-data, include=FALSE}
## Assumes data is stored in "raw-data/" folder under current working directory
data_dir <- "raw-data"
kidney_data <- read_csv(
  file.path(data_dir, "GSE21374-kidney-data.csv.gz"),
  col_types = cols(
    .default = col_double(),
    Patient_ID = col_character()
  )
)

## Glimpse first 10 columns
str(kidney_data[, 1:10])

## Extract gene expression data as matrix X
X <- kidney_data %>% 
  dplyr::select(-Patient_ID, -Reject_Status) %>% 
  as.matrix()
rownames(X) <- kidney_data$Patient_ID
dim(X)
str(X)
X <- scale(X, center = TRUE, scale = TRUE) 

## Extract Reject_Status column as vector
reject_status <- as.factor(kidney_data$Reject_Status)
names(reject_status) <- kidney_data$Patient_ID
length(reject_status)
table(reject_status) # number of 0's (accepts) and 1's (rejects)
```

# Data visualisation

We will first try to visualize the data in order to see if certain patterns can be discovered. 

## Scree plot

We perform an SVD on X:

$$
X = \sum_{k=1}^r\delta_ku_kv_k^T
$$
with $r$ the rank of X, $\delta$ the singular values, $u$ the left singular vectors and $v$ the right singular vectors.

We proceed to plot the singular values (principal components or PC's) against the proportion of the total variance that they account for.

```{r warning=FALSE, cache =TRUE}
svdX <- svd(X)
```


```{r warning=FALSE}
nX <- nrow(X)
r <- ncol(svdX$v)

totVar <- sum(svdX$d^2)/(nX-1)
vars <- data.frame(comp=1:r,var=svdX$d^2/(nX-1)) %>%
  mutate(propVar=var/totVar,cumVar=cumsum(var/totVar))

pVar2 <- vars %>%
  ggplot(aes(x=comp:r,y=propVar)) +
  geom_point() +
  geom_line() +
  xlab("PC") +
  ylab("Proportion of Total Variance")

pVar3 <- vars %>%
  ggplot(aes(x=comp:r,y=cumVar)) +
  geom_point() +
  geom_line() +
  xlab("PC") +
  ylab("Cumulative Proportion of Total Variance")

grid.arrange(pVar2, pVar3, nrow=1)
```

As we can see, even the first PC's don't explain much of the variability, which means we need many PC's to explain most of the variability. 

Nevertheless, the first 2 principal components are used in order to facilitate visualisation of the data.


## Scatterplot 


```{r}
k <- 2
Vk <- svdX$v[,1:k]
Uk <- svdX$u[,1:k]
Dk <- diag(svdX$d[1:k])
Zk <- Uk%*%Dk
colnames(Zk) <- paste0("Z",1:k)


Zk %>%
  as.data.frame %>%
  mutate(Reject_Status = reject_status %>% as.factor) %>%
  ggplot(aes(x= Z1, y = Z2, color = Reject_Status)) +
  geom_point(size = 3)
```

On the scatterplot each dot represents a patient and the colour is the rejection status of that patient, they are plotted against the first PC (x-axis) and the second PC (Y-axis).
We see there is much overlap between the two groups, but the second PC might be interesting with regards to the rejection status, as more red dots can be seen in the upper area of the plot and more blue dots in the lower area.


### Asses loadings 


Because a biplot in this setting will not be interpretable, we only assess the loadings of the first two PC's, for each gene. These loadings represent the contribution of each gene to the PC.

```{r}
par(mfrow = c(1, 2))

hist(Vk[, 1], breaks = 50, xlab = "PC 1 loadings", main = "")
abline(v = c(
  quantile(Vk[, 1], 0.05),
  quantile(Vk[, 1], 0.95)), col = "red", lwd = 2)


hist(Vk[, 2], breaks = 50, xlab = "PC 2 loadings", main = "")
abline(v = c(
  quantile(Vk[, 2], 0.05),
  quantile(Vk[, 2], 0.95)
), col = "red", lwd = 2)
```

Many genes contribute to the first PC's, the vast majority (95%) of these genes are within the two vertical red lines. We can see no distinct outliers, no genes in particular are really driving these PC's. The loadings of the first PC are skewed to the right, the do not follow a normal distribution. There a very few negative loadings compared to positive, meaning most genes will positively influence the first PC when their gene expression is upregulated.



## LDA

A better way to visualize a potential difference between the two rejection statusses is with Fisher's Linear Discriminant Analysis.
We will look for a direction $a$ in the 13699-dimensional space, so that the orthogonal projections of the predictors ($X^Ta$) show a maximized ratio between the SSB (between sum of squares) and the SSE (within sum of squares).

$$
V = ArgMax_a\frac{a^TBa}{a^TWa}
$$

$B$ is the between covariance matrix of X, $W$ is the within covariance matrix of X, and $a^TWa = 1$ in order to have a unique solution


```{r cache = TRUE}
kid_lda <- lda(x = X, grouping = reject_status)
```


```{r}
cols <- c("n" = "red", "t" = "blue")
Vlda <- kid_lda$scaling
Zlda <- X %*% Vlda
par(mfrow = c(1, 1))
boxplot(Zlda ~ reject_status, col = cols, ylab = expression("Z"[1]), 
        main = "Separation of non rejected and rejected kidneys by LDA")
```

With LDA we see a clear distinction between the two groups, with a little overlap.

This shows that we might assume that there is a difference in gene expression between patients who show no rejection of the kidney compared to those whose transplanted kidney is rejected.



# Hypothesis testing

We will test the following hypothesis

$H_{0i}: \mu_{NRi} = \mu_{Ri}$

against the alternative

$H_{1i}: \mu_{NRi} \neq \mu_{Ri}$

for all 13669 genes. 

This will cause a big multiple testing problem, which we will then correct using the Benjamini and Hochberg method with the False Discover Rate (FDR) set at 10%.

## Two-sided two-sample t-test

A two-sided two-sample t-test is executed for all 13669 genes. We have unequal sample sizes $n_0$ and $n_1$ but assume equal variance.

$t=\frac{\overline{X_0}-\overline{X_1}}{S\sqrt{\frac{1}{n_0}+\frac{1}{n_1}}}$

with $n_0=174$ and $n_1 = 76$


```{r}
ttest_results <- t(apply(X, 2, function(x) {
  t_test <- t.test(x ~ reject_status)
  p_val <- t_test$p.value
  stat <- t_test$statistic
  df <- t_test$parameter
  ## Return values in named vector
  c(stat, "p_val" = p_val, df)
}))

head(ttest_results)
```

```{r}
p_vals <- ttest_results[, "p_val"]
hist(
  p_vals,
  breaks = seq(0, 1, by = 0.05), main = "", xlab = "p-value",
  ylim = c(0, 5000)
)

```

This plot shows us that a large proportion of p-values falls under the significance threshold of 0.05. 

```{r}
alpha <- 0.05
sum(p_vals < alpha)
```

This gives us 4012 significant results. Because we perform 13669 simultaneous t-tests at a 0.05 significance level, 0.05 x 13699 = 684.95 results would come up positive if all nullhypotheses were true, meaning we expect 685 false positives to occur if we don't correct for multiple testing.

## BH95

The next step is to correct the multiple testing problem with an FDR set at 10%, using the Benjamini and Hochberg (1995) method. An FDR of 10% means we will tolerate on average 10% false postives among all the positive outcomes:

$FDR = E[\frac{FP}{R}] = E[FDP]$

The BH95 method consists of calculating adjusted p-values, which we then test against our FDR controlled at $\alpha = 0,1$:

$q_{(i)}=min[min_{j=i, ...,m}(\frac{mp_{(j)}}{j}), 1]$



```{r}
fdr <- p.adjust(p_vals, method = "BH")

plot(
  p_vals[order(p_vals)], fdr[order(p_vals)],
  pch = 19, cex = 0.6, xlab = "p-value", ylab = "FDR-adjusted p-value", col = 4
)
abline(a = 0, b = 1)
```

```{r}
sum(fdr < 0.10)
```

When the FDR is controlled at 10%, we still find 3106 significant discoveries. 
We have to take into account that we are testing a subset of 25% of the most variable genes from the original data set. This explains why such a large proportion of genes comes back as possibly having differential expressions.

We can further explore the distribution of these adjusted p-values.

```{r}
fdr_df <- as.data.frame(fdr)
fdr_df$ID <- rownames(fdr_df)

sign_genes <- fdr_df %>%
  filter(fdr_df$fdr < 0.10)
ord_genes <- sign_genes[order(sign_genes$fdr),, drop = FALSE]

head(ord_genes, 10)
```
We created a list with the significant p-values sorted from small to large (here we only show 10 genes with the lowest p-values).

```{r}
ord_genes$logp <- log(ord_genes$fdr)
hist(ord_genes$logp,
     breaks = 50,
     ylim = c(0, 800),
     xlim = c(-40, 5),
     main = "Histogram of the logtransformed significant adjusted p-values")
```

In order to see how these p-values are distributed, we logtransformed them and plotted these values on a histogram. This shows us that there are some very small adjusted p-values. The genes corresponding to these p-values might be interesting to look at. 

```{r}
under10 <- ord_genes %>%
  filter(ord_genes$logp < -10)
nrow(under10)

under30 <- ord_genes %>%
  filter(ord_genes$logp < -30)
nrow(under30)
```


## Local fdr

Another way to explore potential differential expression is by using the local fdr method. This gives us the probability that a gene is a null (gene for which $H_{0i}$ is true, i.e. no differential expression) given a certain gene: $fdr(z) =P[null|z]$. 

If the local fdr is sufficiently small for a given gene, it will be very probable that this gene will be a true positive.

For this method we need to transform the t-statistics into z-scores. 

```{r}
t <- ttest_results[, "t"]
length(t)

z1 <- rep(NA, length(t))
for(i in 0:length(t)){
  z1[i] <- qnorm(pt(t[i], df= 248))
}
mean(z1)  
sd(z1)

```


```{r}
lfdr <- locfdr(z1, plot = 2)
```

This graph shows us that we can expect non-nulls for extreme negative z-values, and (almost) none for positive z-values.

The expected false discovery rate  $Efdr = E_{f1}[fdr(z)]$  is the expected probability of falsely finding a null als a significant result and is a measure for the power of the tests. We want this to be as small as possible.
Here the Efdr is 0.235, we have a 23.5% chance of falsely claiming a significant result. 


```{r}
lfdr1 <- lfdr$fdr
gene_ID <- colnames(X)
lfdr_df <- as.data.frame(cbind(gene_ID, lfdr1))
lfdr_df$lfdr1 <- as.numeric((lfdr_df$lfdr1))
sig <- lfdr_df[fdr_df$ID %in% sign_genes$ID,]
hist(sig$lfdr1)
```

When we now only look at the genes that were returned significant with FDR = 10%, we see that a large proportion of these genes score poorly on a local fdr basis (many are around 100% local fdr), but we also see many genes under 10% local fdr.

We could filter additionally with local fdr set at 10% 

```{r}
Sig_sig <- sig %>%
  filter(sig$lfdr1 < 0.10)
nrow(Sig_sig)
most_sig_p <- sign_genes[sign_genes$ID %in% Sig_sig$gene_ID,]
most_sig_p$lfdr <- Sig_sig$lfdr1
colnames(most_sig_p) <- c("adj_p", "ID", "lfdr")
rownames(most_sig_p) <- NULL
head(most_sig_p)
```

This gives us 522 genes that have at the most a probability of 10% of being a null (no differential expression). The fill list can be seen in the appendix.





# Prediction 

For the third research question we will see if we can find a prediction model that can predict kidney rejection status from gene expression. The problem we encounter when dealing with high dimensional data ($p>n$) is that we have too many variables, which will cause the model to be grossly overfitted and useless for prediction purposes.


3 methods can be used to circumvent this problem:

- Principal Component Regression (PCR)

- Penalized Regression: Ridge 

- Penalized Regression: Lasso

Before we begin applying these methods we divide the original data set in training data and test data. This way the training data is used for building the model, while we keep the test data to evaluate our final model.

```{r}
set.seed(2021)
####
Y <- kidney_data$Reject_Status
n <- nrow(X)
nTrain <- round(0.7*n)
indTrain <- sample(1:n,nTrain)
XTrain <- X[indTrain,]
YTrain <- Y[indTrain]
XTest <- X[-indTrain,]
YTest <- Y[-indTrain]
table(YTest)
table(YTrain)
table(Y)
23/(52+23) 
53/(122+53)
76/250

```

Our randomly chosen test data set has 30.6% rejection cases on a total of 75 cases, training data 30.3% on 175 cases and our original data had 30.4%. This means the same ratio is roughly preserved.


## PCR 

(PCR code is set to eval = FALSE because it was compiled separately and is not compatible with the rest of the code and impossible to adapt it in time. The output can be found in the appendix)

We will reduce our original matrix of predictors $X$ of $p$ dimensions to a new matrix $Z$ of $n$ dimensions. From this new matrix we will only select those PC's that contribute the most with regards to our research question. These are not necessarily the first PC's.



```{r eval=FALSE, include=TRUE}

## Calculate PCA and extract scores
pca_X <- prcomp(XTrain)
pca_var <- pca_X$sdev^2
pca_var_per <- round(pca_var/sum(pca_var)*100, 1)
barplot(pca_var)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)

## cv.glm() requires the response and predictors in one data.frame, so we need
## to combine them back together
fit_data <- data.frame(YTrain, Z)
head(fit_data)

## Example of PC Log. Reg. with all PCs
full_model <- glm(YTrain ~ ., data = fit_data, family = "binomial")
# summary(full_model)
```

### Cross-validation

In order evaluate our model building process by estimating the Expected Test Error: $E_\tau[Err_\tau]=E_{Y^*, X^*, \tau}[(\hat{m}(X^*)-Y^*)^2]$.  We want this expected test error to be as small as possible.

The ETE is estimated using a 4 fold cross-validation method: $CV_k = \frac{1}{k}\sum_{j=1}^k\frac{1}{n_j}\sum_{i\epsilon S_j)}(Y_i-\hat{m}^{-S_j}(x_i))^2$ with $k=4$

This method will be used to select our principal components (and later to estimate the optimal $\lambda$ for the ridge and lasso penalized regression).

```{r eval=FALSE, include=TRUE}
## 4-fold Cross-validation on this one particular model, using AUC and K = 4
full_model_cv <- cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = pROC::auc, K = 4  # note: specify the auc function (from pROC) without`()`!
)

## We'll just use the raw one here
full_model_cv$delta[1] # This is the AUC for this particular model estimated by AUC

## wrap this code in a for-loop and repeat for each number of PCs
cv_auc <- rep(NA, n_PC)
set.seed(12) # seed for reproducibility
for (i in seq_len(n_PC)) {
  ## Prepare fit_data; subset number of PCs to i
  fit_data <- data.frame(YTrain, Z[, 1:i, drop = FALSE])  # use drop = FALSE to avoid problems when subsetting single column
  pcr_mod <- suppressWarnings(
    glm(y ~ ., data = fit_data, family = "binomial")
  )
  
 ## Do 4-fold CV while suppressing Warnings and Messages 
  cv <- suppressWarnings(
    suppressMessages(
      cv.glm(fit_data, pcr_mod, cost = pROC::auc, K = 4)
    )
  )
  cv_auc[i] <- cv$delta[1]
}
names(cv_auc) <- seq_along(cv_auc)
cv_auc
## Finding the optimal nr. of PCs corresponds to finding the max. AUC
optim_nPC <- names(which.max(cv_auc))
optim_nPC

plot(names(cv_auc), cv_auc, xlab = "n PCs", ylab = "AUC", type = "l")
abline(v = optim_nPC, col = "red")
```


### Model evaluation

We evaluate the model by calculating the Area Under the Curve (AUC), specifically for the Receiver Operating Characteristic (ROC) curve. 
This curve maps the threshold c, which is the critical value which separates the negative outcomes from the positive and which decides the sensitivity (P(true positive)) and the specificity (P(ctrue negative)) of a test. This c is plotted against the sensitivity on the y-axis and 1-specificity (P(false positive)) on the x-axis. The larger the area under this curve means, the better the prediction value of a model (a perfect test which has no false positive or false negative outcomes has a AUC of 1). 

```{r eval=FALSE, include=TRUE}
## prediction using Test data
pca_X <- prcomp(XTrain)
# YTrain <- as.factor(YTrain)
# dim(Xtrain)
Z <- pca_X$x
dim(Z)
pca_var <- pca_X$sdev^2
pca_var_per <- round(pca_var/sum(pca_var)*100, 1)
barplot(pca_var)

opt_data <- data.frame(YTrain, Z[, 1:13])
head(opt_data)
table(YTrain)
summary(YTrain)

opt_model <- glm(YTrain ~ ., data = opt_data, family = "binomial")
summary(opt_model)
#The estimators of coefficients that have been obtained (βZ), as stated in the introduction can be multiplied by matrix V to obtain βX.
beta.Z <- as.matrix(opt_model$coefficients[2:14])
V <- as.matrix(pca_X$rotation[,2:14])

# In order to compare the prediction, I am predicting the values based on the βX coefficient estimates calculated before, according to this equation
# βX = V X βZ
beta.X <- V %*% beta.Z
head(beta.X)

pred.test <- as.matrix(XTest)
# head(pred.test)
# head(beta.X)
y.pred.test2 <- pred.test %*% beta.X

plot(y.pred.test2)

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

prob <- logit2prob(y.pred.test2)
XTest$predict <- ifelse(prob > 0.50, "1", "0")
table(YTest)

confusion_mat <- table(YTest, XTest$predict)
rownames(confusion_mat) <- c("obs.0", "obs.1")
colnames(confusion_mat) <- c("pred.0", "pred.1")
confusion_mat
dim(data.frame(prob))

pred <- prediction(prob, YTest)

perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
paste("AUC:", auc)
```



## Ridge

The penalized Ridge regression method consists of adding a penalty term to the usual sum of least squares solution:

$SSE_{pen}=||Y-X\beta||^2 + \lambda||\beta||_2^2$

where $||\beta||_2^2 = \sum_{j=1}^p\beta_j^2$ is the $L_2$ penalty term and $\lambda >0$ is the penalty parameter.



```{r}
ridge <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 0,         # ridge: alpha = 0
  family="binomial")  

plot(ridge, xvar = "lambda")
```

```{r}
cv_ridge <- cv.glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 0,               # ridge: alpha = 0
  type.measure = "class",
    family = "binomial")  

plot(cv_ridge)
```


### Model Evaluation

We now look at the ROC curve for a model using the optimal penalty parameter $\lambda$ 

```{r}

dfRidgeOpt <- data.frame(
  pi = predict(cv_ridge,
    newx = XTest,
    s = cv_ridge$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)


rocridge <-
  dfRidgeOpt  %>%
  ggplot(aes(d = known.truth, m = pi)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

rocridge
```

```{r}
calc_auc(rocridge)
```


## Lasso

The penalize Lasso regression, like the Ridge regression, uses a penalty term, but a different one:

$SSE_{pen}=||Y-X\beta||^2_2 + \lambda||\beta||_1$

with $||\beta||_1= \sum_{j=1}^p|\beta|$

The difference with the Ridge method is that instead of shrinking the $\beta$'s towards zero, they will be set to zero, resulting in less parameters in the model.

```{r}
lasso <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,         # lasso: alpha = 1
  family="binomial")  

plot(lasso, xvar = "lambda", xlim = c(-6,-1.5))
```

```{r}
cv_lasso <- cv.glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,               # lasso: alpha = 1
  type.measure = "class",
    family = "binomial")  

plot(cv_lasso)
```



### Model evaluation

We now look at the ROC curve for a model using the optimal number of parameters and optimal penalty parameter $\lambda$ 

```{r}
cv_lasso$lambda.min
dfLassoOpt <- data.frame(
  pi = predict(cv_lasso,
    newx = XTest,
    s = cv_lasso$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)


roclasso <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pi)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)") +
  scale_x_continuous(breaks = seq(0,1,0.05))+
  scale_y_continuous(breaks = seq(0,1,0.05))

roclasso

```


```{r}
calc_auc(roclasso)
```

## Optimal Model

We choose to continue with the Lasso model, as this model gives us a AUC of 0.85 (with an optimal $\lambda$ of 0.098 and 21 parameters) compared to Ridge where the AUC is a little less 0.83 (despite using all 13669 parameters). For PCR we had a high AUC, but as we tried to evaluate the model using the test data, it gave very bad results. This was probably because instead of manually selecting our PC's, we had taken the first PC's, taking the number that gave us the optimal AUC, not taking into account that these first PC's do not necessarily carry the most information.

## Choosing optimal threshold c for Lasso model

We find the optimal c at a sensitivity rate of 0.77 and a 1-specificity of 0.19. This means we have an 85% chance of correctly calling a true positive, a 19% chance of a false positive and, consequently a 81% chance of correctly calling a negative. As we prefer being sure of our discoveries, and not waste time on false positives, we would like this to be rather low. Another option would be sensitivity of 85%, but then we have 27% chance on false positives, which would be rather high.



# Conclusion

We can assume with some confidence that there is a relation between rejection status and the gene expression of some genes. This could already be visualized with the LDA technique, and was also confirmed with hypothesis testing. We were able to produce a list of significant genes, with an FDR controlled at 10%, and additionally filtered this list with the use of local fdr. This gave us 522 highly significant genes which could be looked at further with relation to rejection status of transplanted kidneys. Additionally,  we were able to produce a pretty good prediction model from this data set using the Lasso method, with a sensitivity of 80% and a specificity of 96%.


# Appendix

## List of 522 most significant genes

```{r}
most_sig_p
```

## PCR


```{r}
## Separate response and predictor variables
y <- factor(kidney_data$Reject_Status)
X <- X <- kidney_data %>% 
  dplyr::select(-Patient_ID, -Reject_Status) %>% 
  as.matrix()
dim(X)

## Calculate PCA and extract scores
pca_X <- prcomp(X)
pca_var <- pca_X$sdev^2
pca_var_per <- round(pca_var/sum(pca_var)*100, 1)
barplot(pca_var)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)

## cv.glm() requires the response and predictors in one data.frame, so we need
## to combine them back together
fit_data <- data.frame(y, Z)
#head(fit_data)

## Example of PC Log. Reg. with all PCs
full_model <- glm(y ~ ., data = fit_data, family = "binomial")
# summary(full_model)

## 4-fold Cross-validation on this one particular model, using AUC and K = 4
full_model_cv <- cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = pROC::auc, K = 4  # note: specify the auc function (from pROC) without`()`!
)

## We'll just use the raw one here
full_model_cv$delta[1] # This is the AUC for this particular model estimated by AUC

## wrap this code in a for-loop and repeat for each number of PCs
cv_auc <- rep(NA, n_PC)
set.seed(12) # seed for reproducibility
for (i in seq_len(n_PC)) {
  ## Prepare fit_data; subset number of PCs to i
  fit_data <- data.frame(y, Z[, 1:i, drop = FALSE])  # use drop = FALSE to avoid problems when subsetting single column
  pcr_mod <- suppressWarnings(
    glm(y ~ ., data = fit_data, family = "binomial")
  )
  
 ## Do 4-fold CV while suppressing Warnings and Messages 
  cv <- suppressWarnings(
    suppressMessages(
      cv.glm(fit_data, pcr_mod, cost = pROC::auc, K = 4)
    )
  )
  cv_auc[i] <- cv$delta[1]
}
names(cv_auc) <- seq_along(cv_auc)
#cv_auc
## Finding the optimal nr. of PCs corresponds to finding the max. AUC
optim_nPC <- names(which.max(cv_auc))
optim_nPC

plot(names(cv_auc), cv_auc, xlab = "n PCs", ylab = "AUC", type = "l")
abline(v = optim_nPC, col = "red")
```

### PCR prediction using Test data

```{r}
## prediction using Test data
pca_X <- prcomp(XTrain)
# YTrain <- as.factor(YTrain)
# dim(Xtrain)
Z <- pca_X$x
dim(Z)
pca_var <- pca_X$sdev^2
pca_var_per <- round(pca_var/sum(pca_var)*100, 1)
barplot(pca_var)

opt_data <- data.frame(YTrain, Z[, 1:13])
head(opt_data)
table(YTrain)
summary(YTrain)

opt_model <- glm(YTrain ~ ., data = opt_data, family = "binomial")
summary(opt_model)
#The estimators of coefficients that have been obtained (βZ), as stated in the introduction can be multiplied by matrix V to obtain βX.
beta.Z <- as.matrix(opt_model$coefficients[2:14])
V <- as.matrix(pca_X$rotation[,2:14])

# In order to compare the prediction, I am predicting the values based on the βX coefficient estimates calculated before, according to this equation
# βX = V X βZ
beta.X <- V %*% beta.Z
#head(beta.X)
pred.test <- XTest
#str(pred.test)
#str(beta.X)
y.pred.test2 <- pred.test %*% beta.X

plot(y.pred.test2)

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

prob <- logit2prob(y.pred.test2)
XTest$predict <- ifelse(prob > 0.50, "1", "0")
table(YTest)

confusion_mat <- table(YTest, XTest$predict)
rownames(confusion_mat) <- c("obs.0", "obs.1")
colnames(confusion_mat) <- c("pred.0", "pred.1")
confusion_mat
dim(data.frame(prob))

pred <- prediction(prob, YTest)

perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
paste("AUC:", auc)
```











</div>


</div>
</div>

</div>

















<script type="text/javascript" src="/d2l/common/math/MathML.js?v=20.21.4.28830 "></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() { D2LMathML.DesktopInit('https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=MML_HTMLorMML','https://s.brightspace.com/lib/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML','130',false); });</script></body></html>